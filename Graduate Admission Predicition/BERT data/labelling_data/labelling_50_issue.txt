0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_-1,10_1,11_-1,12_1,13_1,14_1,15_1

Passion is clearly necessary for a truly great idea to take hold among a people—passion either on the part of the original thinker, the audience, or ideally both. 
The claim that the most lucrative subject matter for inspiring great ideas is “commonplace things” may seem initially to be counterintuitive. 
After all, aren’t great ideas usually marked by their extraordinary character?
While this is true, their extraordinary character is as often as not directly derived from their insight into things that had theretofore gone unquestioned.
While great ideas certainly can arise through seemingly pure innovation.
For example, Big Bang cosmology, which developed nearly all of its own scientific and philosophical precepts through its own process of formation, it is nevertheless equally true that such groundbreaking thought was, and is, still largely a reevaluation of previous assumptions to a radical degree. 
After all, the question of the ultimate nature of the universe, and man’s place in it, has been central to human thought since the dawn of time.
Commonplace things are, additionally, necessary as material for the generation of “the best ideas” since certainly the success among an audience must be considered in evaluating the significance and quality of an idea. 

The advent of Big Bang cosmology, which occured in rudimentary form almost immediately upon Edwin Hubble’s first observations at the Hooker telescope in California during the early 20th century, was the most significant advance in mankind’s understanding of the universe in over 400 years.
The seemingly simple fact that everything in the universe, on the very large scale, is moving away from everything else in fact betrays nearly all of our scientific knowledge of the origins and mechanics of the universe.
This slight, one might even say commonplace, distortion of tint on a handful of photographic plates carried with it the greatest challenge to Man’s general, often religiously reinforced, conception of the nature of the world to an extent not seen since the days of Galileo. 
Not even Charles Darwin’s theory, though it created more of a stir than Big Bang cosmology, had such shattering implications for our conceptions of the nature of our reality. 
Yet it is not significant because it introduced the question of the nature of what lies beyond Man’s grasp.
A tremendous number of megalithic ruins, including the Pyramids both of Mexico and Egypt, Stonehenge, and others, indicate that this question has been foremost on humankind’s collective mind since time immemorial. 
Big Bang cosmology is so incredibly significant in this line of reasoning exactly because of the degree to which it changed the direction of this generally held, constantly pondered, and very ancient train of thought.

Additionally, there is a diachronic significance to the advent of Big Bang cosmology, which is that, disregarding limitations such as the quality of optical devices available and the state of theoretical math, it could have happened at any point in time.
That is to say, all evidence points to roughly the same raw intellectual capacity for homo sapiens throughout our history, our progress has merely depended upon the degree of it that a person happens to inherit, a pace that has been increasing rapidly since the industrial revolution. 
Yet this discovery had to happen at a certain point in time or another—it cannot have been happening constantly or have never happened yet still be present—and this point in time does have its own significance. 
That significance is precisely the fact that the aforementioned advent must have occurred at precisely the point in time at which it truly could have occured—that is to say, it marks the point in our history when we had progressed sufficiently to begin examining, with remarkable substantiated acuity, the workings of the universe across distances that would take millions of human lifetimes to reach or to traverse. 
The point for the success of this advent must necessarily have been, additionally, the point at which the audience concerned was capable and prepared to accept such a radical line of reasoning. 
Both factors, a radical, passionate interpretation of the commonplace and the preparedness to accept such an interpretation, are necessary for the formulation of a truly great idea. 
If the passion is absent from an inquiry by the thinker or by the bulk of an audience, the idea will die out if it comes to fruition at all. If the material is not sufficiently commonplace to be considered by an informed audience of sufficient size, the same two hazards exist.
Given these two factors, the idea must still be found palatable and interesting by the audience if it is to hope to gain a foothold and eventually establish itself in a significant fashion.



0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_-1,10_-1,11_-1,12_-1,13_1,14_1,15_1

The statement linking technology negatively with free thinking plays on recent human experience over the past century.
Surely there has been no time in history where the lived lives of people have changed more dramatically. 
A quick reflection on a typical day reveals how technology has revolutionized the world. Most people commute to work in an automobile that runs on an internal combustion engine. 
During the workday, chances are high that the employee will interact with a computer that processes information on silicon bridges that are .09 microns wide. 
leaving home, family members will be reached through wireless networks that utilize satellites orbiting the earth. 
Each of these common occurrences could have been inconceivable at the turn of the 19th century.

The statement attempts to bridge these dramatic changes to a reduction in the ability for humans to think for themselves. 
The assumption is that an increased reliance on technology negates the need for people to think creatively to solve previous quandaries.
Looking back at the introduction, one could argue that without a car, computer, or mobile phone, the hypothetical worker would need to find alternate methods of transport, information processing and communication. 
Technology short circuits this thinking by making the problems obsolete.

However, this reliance on technology does not necessarily preclude the creativity that marks the human species.
The prior examples reveal that technology allows for convenience. 
The car, computer and phone all release additional time for people to live more efficiently. This efficiency does not preclude the need for humans to think for themselves.
In fact, technology frees humanity to not only tackle new problems, but may itself create new issues that did not exist without technology.
For example, the proliferation of automobiles has introduced a need for fuel conservation on a global scale. 
With increasing energy demands from emerging markets, global warming becomes a concern inconceivable to the horse-and-buggy generation. 
Likewise dependence on oil has created nation-states that are not dependent on taxation, allowing ruling parties to oppress minority groups such as women. 
Solutions to these complex problems require the unfettered imaginations of maverick scientists and politicians.



0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_-1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_-1

In contrast to the statement, we can even see how technology frees the human imagination. 
Consider how the digital revolution and the advent of the internet has allowed for an unprecedented exchange of ideas.
WebMD, a popular internet portal for medical information, permits patients to self research symptoms for a more informed doctor visit.
This exercise opens pathways of thinking that were previously closed off to the medical layman.
With increased interdisciplinary interactions, inspiration can arrive from the most surprising corners.
Jeffrey Sachs, one of the architects of the UN Millenium Development Goals, based his ideas on emergency care triage techniques.
The unlikely marriage of economics and medicine has healed tense, hyperinflation environments from South America to Eastern Europe.

This last example provides the most hope in how technology actually provides hope to the future of humanity. 
By increasing our reliance on technology, impossible goals can now be achieved. 
Consider how the late 20th century witnessed the complete elimination of smallpox. 
This disease had ravaged the human race since prehistorical days, and yet with the technology of vaccines, free thinking humans dared to imagine a world free of smallpox. 
Using technology, battle plans were drawn out, and smallpox was systematically targeted and eradicated.

Technology will always mark the human experience, from the discovery of fire to the implementation of nanotechnology. 
Given the history of the human race, there will be no limit to the number of problems, both new and old, for us to tackle. 
There is no need to retreat to a Luddite attitude to new things, but rather embrace a hopeful posture to the possibilities that technology provides for new avenues of human imagination.


0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1


Whenever people argue that history is a worthless subject or that there is nothing to be gained by just “memorizing a bunch of stupid names and dates,” I simply hold my tongue and smile to myself. 
What I’m thinking is that, as cliche as it sounds, you do learn a great deal from history (and woe to those who fail to learn those lessons). 
It is remarkable to think of the number of circumstances and situations in which even the most rudimentary knowledge of history will turn out to be invaluable. 
Take, for example, the issue at hand here. Is it better for society to instill in future leaders a sense of competition or cooperation? 
Those who have not examined leaders throughout time and across a number of fields might not have the ability to provide a thorough and convincing answer to this question, in spite of the fact that it is crucial to the future functioning of our society.
Looking closely at the question of leadership and how it has worked in the past, I would have to agree that the best way to prepare young people for leadership roles is to instill in them a sense of cooperation. 

Let us look first at those leaders who have defined themselves based on their competitiveness.
Although at first glance it may appear that a leader must have a competitive edge in order to gain and then maintain a leadership position, I will make two points on this subject. 
First, the desire to compete is an inherent part of human nature; that is, it is not something that needs to be “instilled” in young people. Is there anyone who does not compete in some way or another every single day? 
You try to do better than others in your school work or at the office, or you just try to do better than yourself in some way, to push yourself. 
When societies instill competitiveness in their leaders, it only leads to trouble. 
The most blatant example in this case is Adolf Hitler, who took competition to the very extreme, trying to prove that his race and his country were superior to all. 
We do not, however, need to look that far to find less extreme examples (i.e., Hitler is not the extreme example that disproves the rule). 
The recent economic meltdown was caused in no large part by the leaders of American banks and financial institutions who were obsessed with competing for the almighty dollar. 
Tiger Woods, the ultimate competitor in recent golfing history and in many ways a leader who brought the sport of golf  to an entirely new level, destroyed his personal life (and perhaps his career ˉˉ still yet to be determined) by his overreaching sense that he could accomplish anything, whether winning majors or sleeping with as many women as possible.
His history of competitiveness is well documented; his father pushed him from a very early age to be the ultimate competitor. 
It served him well in some respects, but it also proved to be detrimental and ultimately quite destructive.

Leaders who value cooperation, on the other hand, have historically been less prone to these overreaching, destructive tendencies. A good case in point would be Abraham Lincoln. 
Now, I am sure at this point you are thinking that Lincoln, who served as President during the Civil War and who refused to compromise with the South or allow secession, could not possibly be my model of cooperation! Think, however, of the way Lincoln structured his Cabinet.
He did not want a group of “yes men” who would agree with every word he said, but instead he picked people who were more likely to disagree with his ideas.
And he respected their input, which allowed him to keep the government together in the North during a very tumultuous period (to say the least).
My point in choosing the Lincoln example is that competitiveness and conflict may play better to the masses and be more likely to be recorded in the history books, but it was his cooperative nature that allowed him to govern effectively. 
Imagine if the CEO of a large company were never able to compromise and insisted that every single thing be done in exactly her way.
Very quickly she would lose the very people that a company needs in order to survive, people with new ideas, people ready to make great advances.
Without the ability to work constructively with those who have conflicting ideas, a leader will never be able to strike deals, reach consensus, or keep an enterprise on track. 
Even if you are the biggest fish in the pond, it is difficult to force your will on others forever; eventually a bigger fish comes along (or the smaller fish team up against you!).

In the end, it seems most critical for society to instill in young people a sense of cooperation. 
In part this is true because we seem to come by our competitive side more naturally, but cooperation is more often something we struggle to learn (just think of kids on the playground). 
And although competitive victory is more showy, more often than not the real details of leadership come down to the ability to work with other people, to compromise and cooperate.
Getting to be President of the United States or the managing director of a corporation might require you to win some battles, but once you are there you will need diplomacy and people-skills. 
Those can be difficult to learn, but if you do not have them, you are likely to be a short-lived leader.



0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1


The statement contends that if a goal is worthy enough then any means taken to attain it are justifiable. 
The issue stated is very subjective in nature as the phrase "any means taken" can be interpreted in many ways. 
Also, it depends on the goal that one is trying to achieve, categorizing it in two broad areas: Personal and Societal.

If a goal relates to the society as a whole, or if it is going to benefit an entire nation, then any means taken to achieve it are justifiable.
For example, Narendra Modi, prime minister of India, had a vision of developing Gujarat, a state in India. 
This vision, and if turns out to be successful can be considered as worthy for an entire state of people. 
Hence, measures taken like blocking roads for metro construction, traffic became a big problem. 
But this problem wasn't bigger than the goal, therefore it is justifiable. Along with this, other means that included collecting extra taxes from taxpayers for the upliftment and well-being of indigents, living in villages far away from city area, was done to achieve a goal of mollifying the electricity cut-off and water shortage problems that were faced by them.

Conversely, if a goal can be categorised as personal, then there can be several factors to be considered before coming to a conclusion whether any means taken to chieve the same are justifiable or not. 
Considering a primary school student having a goal of achieveing good marks in a particular examination, the first and easiest solution that comes to his/her mind is copying. 
This goal can be achieved in several other ways like studying hard, or say understanding the core concepts by attaining extra classes. 
Hence, the means taken by that primary level kid is not justifiable.

Additionally, a personal level goal like achieving a target in business, which would at first seem plausible, but if achieved, can be beneficial to many, and thus can be considered as worthy enough that any steps taken to attain it are justifiable.
For example, if sales of a certain product needs to be increased, then various measures taken can include, increased price of that product or increased level of awareness for that product through advertisment, which are very basic but if it includes something like removing the unworthy staff of the company and replacing it with the better ones, that could cause problem to a few, but it is justifibale as long as the goal is concerned.

To recapitulate, if a goal is worthy enough, any means taken to attain it are justifiable, unless ans until it doesn't cause harm to a potential level. 
Along with it, several factors should be considered before coming to a direct conclusion.



0_1,1_1,2_-1,3_1,4_-1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_1,13_1,14_1,15_1


Do we learn more from people whose ideas we share in common than from those whose ideas contradict ours?
The speaker claims so, for the reason that disagreement can cause stress and inhibit learning. 
I concede that undue discord can impede learning. Otherwise, in my view we learn far more from discourse and debate with those whose ideas we oppose than from people whose ideas are in accord with our own. 
Admittedly, under some circumstances disagreement with others can be counterproductive to learning. 
For supporting examples one need look no further than a television set. On today's typical television or radio talk show, disagreement usually manifests itself in meaningless rhetorical bouts and shouting matches, during which opponents vie to have their own message heard, but have little interest either in finding common ground with or in acknowledging the merits of the opponent's viewpoint.
Understandably, neither the combatants nor the viewers learn anything meaningful. 
In fact, these battles only serve to reinforce the predispositions and biases of all concerned. The end result is that learning is that learning is impeded.

Disagreement can also inhibit learning when two opponents disagree on fundamental assumptions needed for meaningful discourse and debate.
For example, a student of paleontology learns little about the evolution of an animal species under current study by debating with an individual whose religious belief system precludes the possibility of evolution to begin with. 
And, economics and finance students learn little about the dynamics of a laissez-faire system by debating with a socialist whose view is that a centrv2ized power should control all economic activity. 
Aside from the foregoing two provisos, however, I fundamentally disagree with the speaker's claim. 
Assuming common ground between two rational and reasonable opponents willing to debate on intellectual merits, both opponents stand to gain much from that debate. 
Indeed it is primarily through such debate that human knowledge advances, whether at the personal, community, or global level. 
At the personal level, by listening to their parents' rationale for their seemingly oppressive rules and policies teenagers can learn how certain behaviors naturally carry certain undesirable consequences. 
At the same time, by listening to their teenagers concerns about autonomy and about peer pressures parents can learn the valuable lesson that effective parenting and control are two different things. 
At the community level, through dispassionate dialogue an environmental activist can come to understand the legitimate economic concerns of those whose jobs depend on the continued profitable operation of a factory. 
Conversely, the latter might stand to learn much about the potential public health price to be paid by ensuring job growth and a low unemployment rate. 
Finally, at the global level, two nations with opposing or economic interests can reach mutually beneficial agreements by striving to understand the other's legitimate concerns for its national security, its political sovereignty, the stability of its economy and currency, and so forth.

In sum, unless two opponents in a debate are each willing to play on the same field and by the same rules, I concede that disagreement can impede learning. 
Otherwise, reasoned discourse and debate between people with opposing viewpoints is the very foundation upon which human knowledge advances. 
Accordingly, on balance the speaker is fundamentally correct.



0_1,1_1,2_1,3_-1,4_-1,5_0,6_-1,7_-1,8_0,9_-1,10_1,11_-1,12_1,13_1,14_1,15_1


I strongly agree with the assertion that significant advances in knowledge require expertise from various fields. 
The world around us presents a seamless web of physical and anthropogenic forces, which interact in ways that can be understood only in the context of a variety of disciplines. 
Two examples that aptly illustrate this point involve the fields of cultural anthropology and astronomy.
Consider how a cultural anthropologist's knowledge about an ancient civilization is enhanced not only by the expertise of the archeologist--who unearths the evidence—but ultimately by the expertise of biochemists, geologists, linguists, and even astronomers. 
By analyzing the hair, nails, blood and bones of mummified bodies, biochemists and forensic scientists can determine the life expectancy, general well-being, and common causes of death of the population.
These experts can also ensure the proper preservation of evidence found at the archeological site. 
A geologist can help identify the source and age of the materials used for tools, weapons, and structures--thereby enabling the anthropologist to extrapolate about the civilization's economy, trades and work habits, life styles, extent of travel and mobility, and so forth.
Linguists are needed to interpret hieroglyphics and extrapolate from found fragments of writings. 
And an astronomer can help explain the layout of an ancient city as well as the design, structure and position of monuments, tombs, and temples--since ancients often looked to the stars for guidance in building cities and structures.

An even more striking example of how expertise in diverse fields is needed to advance  knowledge involves the area of astronomy and space exploration. 
Significant advancements in our knowledge of the solar system and the universe require increasingly keen tools for observation and measurement. 
Telescope technology and the measurement of celestial, distances, masses, volumes, and so forth, are the domain of astrophysicists. 
These advances also require increasingly sophisticated means of exploration. Manned and unmanned exploratory probes are designed by mechanical, electrical, and computer engineers.
And to build and enable these technologies requires the acumen and savvy of business leaders, managers, and politicians. 
Even diplomats might play a role--insofar as major space projects require intemafional cooperative efforts among the world's scientists and governments. 
And ultimately it is our philosophers whose expertise helps provide meaning to what we learn about our universe. 
In sum, no area of intellectual inquiry operates in a vacuum. Because the sciences are In extricable related, to advance our knowledge in any one area we must understand the interplay among them all. 
Moreover, it is our non-scientists who make possible the science and who bring meaning to what we learn from it.    



0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1


The speaker would prefer a national curriculum for all children up until college instead of allowing schools in different regions the freedom to decide on their own curricula. 
I agree Insofar as some common core curriculum would serve useful purposes for any nation. 
At the same time, however, individual states and communities should have some freedom to augment any such curriculum as they see fit; otherwise, a nation's educational system might defeat its own purposes in the long tenn.
 
A national core curriculum would be beneficial to a nation in a number of respects.
First of all, by providing all children with fundamental skills and knowledge, a common core curriculum would help ensure that our children grow up to become reasonably informed, productive members of society. 
In addition, a common core curriculum would provide a predictable foundation upon which college administrators and faculty could more easily build curricula and select course materials for freshmen that are neither below nor above their level of educational experience. 
Finally, a core curriculum would ensure that all school-children are taught core values upon which any democratic society depends to thrive, and even survive--values such as tolerance of others with different viewpoints, and respect for others.
 
However, a common curriculum that is also an exdusive one would pose certain problems, which might outweigh the benefits, noted above. 
First of all, on what basis would certain course work be included or excluded, and who would be the final decision- maker? 
In all likelihood these decisions would be in the hands of federal legislators and regulators, who are likely to have their own quirky notions of what should and should not be taught to children--notions that may or may not reflect those of most communities, schools, or parents. 
Besides, government officials are notoriously susceptible to influence-peddling by lobbyists who do not have the best interests of society's children in mind. 
Secondly, an official, federally sanctioned curriculum would facilitate the dissemination of propaganda and other dogma which because of its biased and one-sided nature undermines the very purpose of true education: to enlighten. 
I can easily foresee the banning of certain text books, programs, and websites which provide information and perspectives that the government might wish to suppress--as some sort of threat to its authority and power. 
Although this scenario might seem far-fetched, these sorts of concerns are being raised already at the state level.
 
Thirdly, the inflexible nature of a uniform national curriculum would preclude the inclusion of programs, courses, and materials that are primarily of regional or local significance.
For example, California requires children at certain grade levels to learn about the history of particular ethnic groups who make up the state's diverse population. 
A national curriculum might not allow for this feature, and California's youngsters would be worse off as a result of their ignorance about the traditions, values, and cultural contributions of all the people whose citizenship they share.
 
Finally, it seems to me that imposing a uniform national curriculum would serve to undermine the authority of parents over their own children, to even a greater extent than uniform state laws currently do. 
Admittedly, laws requiring parents to ensure that their children receive an education that meets certain minimum standards are well-justified, for the reasons mentioned earlier. 
However, when such standards are imposed by the state rather than at the community level parents are left with far less power to participate meaningfully in the decision-making process. 
This problem would only be exacerbated were these decisions left exclusively to federal regulators.
In the final analysis, homogenization of elementary and secondary education would amount to a double-edged sword. 
While it would serve as an insurance policy against a future populated with illiterates and ignoramuses, at the same time it might serve to obliterate cultural diversity and tradition. 
The optimal federal approach, in my view, is a balanced one that imposes a basic curriculum yet leaves the rest up to each state--or better yet, to each community.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_1,9_1,10_1,11_-1,12_1,13_1,14_1,15_1


According to the speaker, the video recording is a more important means of document hag contemporary life than a written record because video recordings are more accurate and convincing.
Although I agree that a video provides a more objective and accurate record of an event's spatial aspects, there is far more to document ha life than what we see and hear. 
Thus the speaker overstates the comparative significance of video as a documentary tool. 
 
For the purpose of documenting temporal, spatial events and experiences, I agree that a video record is usually more accurate and more convincing than a written record.
It is impossible for anyone, no matter how keen an observer and skilled a journalist, to recount has complete and objective detail such events as the winning touchdown at the Super Bowl, a Ballanchine ballet, the Tournament of Roses Parade, or the scene at the intersection of Florence and Normandy streets during the 1992 Los Angeles riots. 
Yet these are important events in contemporary life the sort of events we might put ha a time capsule for the purpose of capturing our life and times at the turn of this millennium.
 
The growing documentary role of video is not limited to seminal events like those described above.
 Video surveillance cameras are objective witnesses with perfect memories. 
Thus they can play a vital evidentiary role in legal proceedings--such as those involving robbery, drug trafficking, police misconduct, motor vehicle violations, and even malpractice in a hospital operating room. 
Indeed, whenever moving images are central to an event the video camera is superior to the written word. 
A written description of a hurricane, tornado, or volcanic eruption cannot convey its immediate power and awesome nature like a video record. 
A diary entry cannot "replay" that wedding reception, dance recital, or surprise birthday party as accurately or objectively as a video record. 
And a real estate brochure cannot inform about the lighting, spaciousness, or general ambiance of a featured property nearly as effectively as a video. 

Nonetheless, for certain other purposes written records are advantageous to and more appropriate than video records. 
For example, certain legal matters are best left to written documentation: video is of no practical use ha documenting the terms of a complex contractual agreement, an incorporation, or the establishment of a trust. 
And video is of little use when it comes to documenting a person's subjective state of mind, impressions, or reflections of an event or experience. 
Indeed, to the extent that personal interpretation adds dimension and richness to the record, written documentation is actually more important than video.
 
Finally, a video record is of no use in documenting statistical or other quantitative information. 
Returning to the riot example mentioned earlier, imagine relying on a video to document the financial loss to store owners, the number of police and firefighters involved, and so forth. 
Complete and accurate video documentation of such information would require video cameras at every street corner and in every aisle of every store.
 
In sum, the speaker's claim overstates the importance of video records, at least to some extent. 
When it comes to capturing, storing, and recalling temporal, spatial events, video records are inherently more objective, accurate, and complete. 
However, what we view through a camera lens provides only one dimension of our life and times; written documentation will always be needed to quantify, demystify, and provide meaning to the world around us.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_-1,10_-1,11_1,12_1,13_1,14_1,15_1


I agree with the speaker that it is sometimes necessary, and even desirable, for political leaders to withhold information from the public. 
A contrary view would reveal a naiveted about the inherent nature of public politics, and about the sorts of compromises on the part of well-intentioned political leaders necessary in order to further the public's ultimate interests.

Nevertheless, we must not allow our political leaders undue freedom to with-hold information, otherwise, we risk sanctioning demagoguery and undermining the philosophical underpinnings of any democratic society.
One reason for my fundamental agreement with the speaker is that in order to gain the opportunity for effective public leadership, a would-be leader must first gain and maintain political power. 
In the game of politics, complete forthrightness is a sign of vulnerability and naivete, neither of which earn a politician respect among his or her opponents, and which those opponents will use to every advantage to defeat the politician. 
In my observation some measure of pandering to the electorate is necessary to gain and maintain political leadership.

For example, were all politicians to fully disclose every personal foibles, character flaw, and detail concerning personal life, few honest politicians would ever by elected. While this view might seem cynical, personal scandals have in fact proven the undoing of many a political career; thus I think this view is realistic.
Another reason why I essentially agree with the speaker is that fully disclosing to the public certain types of information would threaten public safety and perhaps even national security. 
For example, if the President were to disclose the government's strategies for thwarting specific plans of an international terrorist or a drug trafficker, those strategies would surely fail and the public's health and safety would be compromised as a result. 
Withholding information might also be necessary to avoid public panic. While such cases are rare, they do occur occasionally. For example, during the first few hours of the new millennium the U.S.

Pentagon's missile defense system experienced a Y2K- related malfunction. 
This fact was withheld from the public until later in the day, once the problem had been solved; and legitimately so, since immediate disclosure would have served no useful purpose and might even have resulted in mass hysteria.

Having recognized that withholding information from the public is often necessary to serve the interests of that public, legitimate political leadership nevertheless requires forthrightness with the citizenry as to the leader's motives and agenda. 
History informs us that would-be leaders who lack such forthrightness are the same ones who seize and maintain power either by brute force or by demagoguery--that is, by deceiving and manipulating the citizenry. 
Paragons such as Genghis Khan and Hitler, respectively, come immediately to mind. Any Democratic society should of course abhor demagoguery, which operates against the Democratic principle of government by the people. 
Consider also less egregious examples, Such as President Nixon's withholding of information about his active role in the Watergate cover-up. 
His behavior demonstrated a concern for self- interest above the broader interests of the democratic system that granted his political authority in the first place.

In sum, the game of politics calls for a certain amount of disingenuousness and lack of forthrightness that we might otherwise characterize as dishonesty. 
And such behavior is a Necessary means to the final objective of effective political leadership. 
Nevertheless, in any democracy a leader who relies chiefly on deception and secrecy to preserve that leadership, toAdvance a private agenda, or to conceal selfish motives, betrays the democracy-and ends up forfeiting the political game.



0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_-1,10_1,11_1,12_1,13_-1,14_1,15_1



The speaker's claim is actually threefold: (1) ensuring the survival of large cities and, in turn, that of cultural traditions, is a proper function of government; 
(2) government support is needed for our large dries and cultural traditions to survive and thrive; 
(3) cultural traditions are preserved and generated primarily in our large cities. I strongly disagree with all three claims.


First of all, subsidizing cultural traditions is not a proper role of government. Admittedly, certain objectives, such as public health and safety, are so essential to the survival of large dries and of nations that government has a duty to ensure that they are met.
However, these objectives should not extend tenuously to preserving cultural traditions. 
Moreover, government cannot possibly play an evenhanded role as cultural patron. Inadequate resources call for restrictions, priorities, and choices. 
It is unconscionable to relegate normative decisions as to which cities or cultural traditions are more deserving, valuable, or needy to a few legislators, whose notions about culture might be misguided or unrepresentative of those of the general populace.
Also, legislators are all too likely to make choices in favor of the cultural agendas of their home towns and states, or of lobbyists with the most money and influence.

Secondly, subsidizing cultural traditions is not a necessary role of government. 
A lack of private funding might justify an exception. 
However, culture--by which I chiefly mean the fine arts--has always depended primarily on the patronage of private individuals and businesses, and not on the government. 
The Medicis, a powerful banking family of Renaissance Italy supported artists Michelangelo and Raphael. 
During the 20th Century the primary source of cultural support were private foundations established by industrial magnates Carnegie, Mellon, Rockefeller and Getty. 
And tomorrow cultural support will come from our new technology and media moguls----including the likes of Ted Turner and Bill Gates.
In short, philanthropy is alive and well today, and so government need not intervene to ensure that our cultural traditions are preserved and promoted.

Finally, and perhaps most importantly, the speaker unfairly suggests that large cities serve as the primary breeding ground and sanctuaries for a nation's cultural traditions. Today a nation's distinct cultural traditions--its folk art, crafts, traditional songs, customs and ceremonies--burgeon instead in small towns and rural regions. Admittedly, our cities do serve as our centers for "high art"; big cities are where we deposit, display, and boast the world's preeminent art, architecture, and music. 
But big-city culture has little to do any- more with one nation's distinct cultural traditions. 
After all, modern cities are essentially multicultural stew pots; accordingly, by assisting large cities a government is actually helping to create a global culture as well to subsidize the traditions of other nations' cultures.

In the final analysis, government cannot philosophically justify assisting large cities for the purpose of either promoting or preserving the nation's cultural traditions; nor is government assistance necessary toward these ends. 
Moreover, assisting large cities would have little bearing on our distinct cultural traditions, which abide elsewhere.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

I agree that it would serve the interests of all nations to establish a global university for the purpose of solving the world's most persistent social problems. 
Nevertheless, such a university poses certain risks which all participating nations must be careful to minimize--or risk defeating the university's purpose.  
One compelling argument in favor of a global university has to do with the fact that its faculty and students would bring diverse cultural and educational perspectives to the problems they seek to solve.

It seems to me that nations can only benefit from a global university where students learn ways in which other nations address certain soda] problems-successfully or not.
It might be tempting to think that an overly diversified academic community would impede communication among students and faculty. 
However, in my view any such concerns are unwarranted, especially considering the growing awareness of other peoples and cultures which the mass media, and especially the Internet, have created.
Moreover, many basic principles used to solve enduring social problems know no national boundaries; thus a useful insight or discovery can come from a researcher or student from any nation.

Another compelling argument for a global university involves the increasingly global nature of certain problems. 
Consider, for instance, the depletion of atmospheric ozone, which has waned the Earth to the point that it threatens the very survival of the human species. 
Also, we are now learning that dear-cutting the world's rainforests can set into motion a chain of animal extinction that threatens the delicate balance upon which all animals—including humans--depend. 
Also consider that a financial crisis---or a political crisis or natural disaster in one country can spell trouble for foreign companies, many of which are now multinational in that they rely on the labor forces, equipment, and raw materials of other nations. Environmental, economic, and political problems such as these all carry grave social consequences--increased crime, unemployment, insurrection, hunger, and so forth. Solving these problems requires global cooperation--which a global university can facilitate.

Notwithstanding the foregoing reasons why a global university would help solve many of our most pressing social problems, the establishment of such a university poses certain problems of its own which must be addressed in order that the university can achieve its objectives.
First, participant nations would need to overcome a myriad of administrative and political impediments. 
All nations would need to agree on which problems demand the university's attention and resources, which areas of academic research are worthwhile, as well as agreeing on policies and procedures for making, enforcing, and amending these decisions. Query whether a functional global university is politically feasible, given that sovereign nations naturally wish to advance their own agendas.

A second problem inherent in establishing a global university involves the risk that certain intellectual and research avenues would become officially sanctioned while others of equal or greater potential value would be discouraged, or perhaps even proscribed. 
A telling example of the inherent danger of setting and enforcing official research priorities involves the Soviet government's attempts during the 1920s to not only control the direction and the goals of its scientists' research but also to distort the outcome of that research---ostensibly for the greatest good of the greatest number of people. 
Not surprisingly, during this time period no significant scientific advances occurred under the auspices of the Soviet government. 
The Soviet lesson provides an important caveat to administrators of a global university: Significant progress in solving pressing social problems requires an open mind to all sound ideas, approaches, and theories---krespective of the ideologies of their proponents.

A final problem with a global university is that the world's preeminent intellectual talent might be drawn to the sorts of problems to which the university is charged with solving, while parochial social problem go unsolved. 
While this is not reason enough not to establish a global university, it nevertheless is a concern that university administrators and participant nations must be aware of in allocating resources and intellectual talent.

To sum up, given the increasingly global nature or the world's social problems, and the
Escalating costs of addressing these problems, a global university makes good sense. And, since all nations would have a common interest in seeing this endeavor succeed, my intuition is that participating nations would be able to overcome whatever procedural and political obstacles that might stand in the way of success. 
As long as each nation is careful not to neglect its own unique social problems, and as long as the university's administrators are careful to remain open-minded about the legitimacy and potential value of various avenues of intellectual inquiry and research, a global university might go a long way toward solving many of the world's pressing social problems


0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_1,13_1,14_1,15_1


The speaker asserts that governments of countries where lesser-known languages are   spoken should intervene to prevent these languages from becoming extinct. 
I agree in so far a country's indigenous and distinct languages should not be abandoned and forgot ten altogether. 
At some point, however, I think cultural identity should yield to the more practical considerations of day-to-day life in a global society..

On the one hand, the indigenous language of any geographical region is part-and-parcel of the cultural heritage of the region's natives.
In my observation we humans have a basic psychological need for individual identity, which we define by way of our membership in distinct cultural groups.
A culture defines itself in various ways--by its unique traditions, rituals, mores, attitudes and beliefs, but especially language. 
Therefore, when a people's language becomes extinct the result is a diminished sense of pride, dignity, and self- worth.

One need look no further than continental Europe to observe how people cling tenaciously to their distinct languages, despite the fact that there is no practical need for them anymore.
And on the other side of the Atlantic Ocean, the French Canadians stubbornly insist on French as their official language, for the sole purpose of preserving their distinct cultural heritage. 
Even where no distinct language exists, people will invent one to gain a sense of cultural identity, as the emergence of the distinct Ebonic can’t among today's African Americans aptly illustrates.
In short, people resist language assimilation because of a basic human need to be part of a distinct cultural group.

Another important reason to prevent the extinction of a language is to preserve the distinct ideas that only that particular language can convey. 
Certain Native American and Oriental languages, for instance, contain words symbolizing spiritual and other abstract concepts that only these cultures embrace. 
Thus, in some cases to lose a language would be to abandon cherished beliefs and ideas that can be conveyed only through language.

On the other hand, in today's high-tech world of satellite communications, global mobility, and especially the Internet, language barriers serve primarily to impede cross-cultural communication, which in turn impedes international commerce and trade. 
Moreover, language barriers naturally breed misunderstanding, a certain distrust and, as a result, discord and even war among nations. 
Moreover, in my view the extinction of all but a few major languages is inexorable--as supported by the fact that the Internet has adopted English as its official language. 
Thus by intervening to preserve a dying language a government might be deploying its resources to fight a losing battle, rather than to combat more pressing social problems--such as hunger, homelessness, disease and ignorance--that plague nearly every society today.

In sum, preserving indigenous languages is, admittedly, a worthy goal; maintaining its own distinct language affords a people a sense of pride, dignity and self-worth. 
Moreover, by preserving languages we honor a people's heritage, enhance our understanding of history, and preserve certain ideas that only some languages properly convey. 
Nevertheless, the economic and political drawbacks of language barriers outweigh the benefits of preserving a dying language. 
In the final analysis, government should devote its time and resources elsewhere, and leave it to the people themselves to take whatever steps are needed to preserve their own distinct languages.


0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_-1,11_-1,12_1,13_1,14_1,15_1

Do modern luxuries serve to undermine our true strength and independence as individuals? 
The speaker believes so, and I tend to agree. 
Consider the automobile, for example. Most people consider the automobile a necessity rather than a luxury; yet it is for this very reason that the automobile so aptly supports the speaker's point. 
To the extent that we depend on cars as crutches, they prevent us from becoming truly independent and strong in character as individuals.

Consider first the effect of the automobile on our independence as individuals. In some respects the automobile serves to enhance such independence. 
For example, cars make it possible for people in isolated and depressed areas without public transportation to become more independent by pursing gainful employment outside their communities.
And teenagers discover that owning a car, or even borrowing one on occasion, affords them a needed sense of independence from their parents.

However, cars have diminished our independence in a number of more significant respects.
We've grown dependent on our cars for commuting to work. We rely on them like crutches for short trips to the corner store, and for carting our children to and from school. 
Moreover, the car has become a means not only to our assorted physical destinations but also to the attainment of our socioeconomic goals, insofar as the automobile has become a symbol of status. 
In fact, in my observation many, if not most, working professionals willingly undermine their financial security for the sake of being seen driving this year's new SUV or luxury sedan. 
In short, we've become slaves to the automobile.

Consider next the overall impact of the automobile on our strength as individuals, by which I mean strength of character, or mettle. 
I would be hard-pressed to list one way in which the automobile enhances one's strength of character. 
Driving a powerful SUV might afford a person a feeling and appearance of strength, or machismo. 
But this feeling has nothing to do with a person's true character.

In contrast, there is a certain strength of character that comes with eschewing modern
Conveniences such as cars, and with the knowledge that one is contributing to a cleaner and quieter environment, a safer neighborhood, and arguably a more genteel society. 
Also, alternative modes of transportation such as bicycling and walking are forms of exercise which require and promote the virtue of self-discipline. 
Finally, in my observation people who have forsaken the automobile spend more time at home, where they are more inclined to prepare and even grow their own food, and to spend more time with their families.
The former enhances one's independence; the latter enhances the integrity of one's values and the strength of one's family.

To sum up, the automobile helps illustrate that when a luxury becomes a necessity it can sap our independence and strength as individuals.
Perhaps our society is better off, on balance, with such "luxuries"; after all, the automobile industry has created countless jobs, raised our standard of living, and made the world more interesting. 
However, by becoming slaves to the automobile we trade off a certain independence and inner strength.



0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_1,10_-1,11_-1,12_1,13_1,14_1,15_1

The speaker claims that most cultures encourage conformity at the expense of individuality and as a result most people conform for fear of being excluded.
While I find the second prong of this dual claim well supported overall by empirical evidence, I take exception with the first prong; aside from the cultures created by certain oppressive political regimes, no culture need "encourage" its members to conform to prevailing ways of thought and behavior; in fact, all the evidence shows that cultures attempt to do just the opposite.

As a threshold matter, it is necessary to distinguish between conformity that an oppressive ruling state imposes on its own culture and conformity in a free democratic society. 
In the former case, people are not only encouraged but actually coerced into suppressing individual personality; and indeed these people are afraid to think and behave differently--but not for fear of being excluded but rather for fear of punishment and persecution by the state. The modern Communist and Fascist regimes are fitting examples. 
With respect to free democratic societies, it might be tempting to dismiss the speaker's dual claim out of hand. 
After all, true democratic states are predicated on individual freedoms---of choice, speech, expression, religion, and so forth. 
Ostensibly, these freedoms serve to promote individuality, even non-conformity, in our personas, our lifestyles, and our opinions and attitudes.

Yet, one look at any democratic society reveals a high degree of conformity among its members. 
Every society has its own bundle of values, customs, and mores which most of its members share. 
Admittedly, within any culture springs up various subcultures which try todistinguish themselves by their own distinct values, customs, and mores. 
In the U.S., for instance, African-Americans have developed a distinct dialect, known as Ebonics, and a distinct body language and attitude which affords them a strong sub-cultural identity of their own. 
Yet, the undeniable fact is that humans, given the actual freedom to either conform or not conform, choose to think and behave in ways similar to most people in their social group---however we define that group.

Nor is there much empirical evidence of any cultural agenda, either overt or covert, to encourage conformity in thought and behavior among the members of any culture. 
To the contrary, the predominant message in most cultures is that people should cultivate their individuality. 
Consider, for example, the enduring and nearly ubiquitous icon of the ragged individualist, who charts his or her own course, bucks the trend, and achieves notoriety through individual creativity, imagination, invention, or entrepreneurship. 
Even our systems of higher education seem to encourage individualism by promoting and cultivating critical and independent thought among its students.

Yet, all the support for forging one's one unique persona, career, lifestyle, opinions, and even belief system, turns out to be hype. 
In the final analysis, most people choose to conform. 
And understandably so; after all, it is human nature to distrust, and even shun, others who are too different from us. 
Thus to embrace rugged individualism is to risk becoming an outcast, the natural consequence of which is to lLmit one's socioeconomic and career opportunities. 
This prospect suffices to quell our yearning to be different; thus the speaker is correct that most of us resign ourselves to conformity for fear of being left behind by our peers. 
Admittedly, few cultures are without rugged individualists----the exceptional artists, inventors, explorers, social reformers, and entrepreneurs who embrace their autonomy of thought and behavior, then test their limits.
And paradoxically, it is the achievements of these notable non-conformists that are responsible for most cultural evolution and progress. 
Yet such notables are few and far between in what is otherwise a world of insecure, even fearful, cultural conformists.

To sum up, the speaker is correct that most people choose to conform rather than behave and think in ways that run contrary to their culture's norms, and that fear of being exuded lies at the heart of this choice. 
Yet, no culture need encourage conformity; most humans recognize that there is safety of numbers, and as a result freely choose conformity over the risks, and potential rewards, of non-conformity.


0_1,1_1,2_1,3_1,4_1,5_1,6_-1,7_-1,8_1,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

According to this statement, each person has a duty to not only obey just laws but also disobey unjust ones.
In my view this statement is too extreme, in two respects. 
First, it wrongly categorizes any law as either just or unjust; and secondly, it recommends an ineffective and potentially harmful means of legal reform. 

First, whether a law is just or unjust is rarely a straightforward issue. The fairness of any law
Depends on one's personal value system. 
This is especially true when it comes to personal Freedoms.
Consider, for example, the controversial issue of abortion. Individuals with particular religious beliefs tend to view laws allowing mothers an abortion choice as unjust, while individuals with other value systems might view such laws as just.

The fairness of a law also depends on one's personal interest, or stake, in the legal issue at
hand. 
After all, in a democratic society the chief function of laws is to strike a balance among competing interests. 
Consider, for example, a law that regulates the toxic effluents a certain factory can emit into a nearby river. 
Such laws are designed chiefly to protect public health. But complying with the regulation might be costly for the company; the factory might be forced to lay off employees or shut down altogether, or increase the price of its products to compensate for the cost of compliance. 
At stake are the respective interests of the company's owners, employees, and customers, as well as the opposing interests of the region's residents whose health and safety are impacted. 
In short, the fairness of the law is subjective, depending largely on how one's personal interests are affected by it.

The second fundamental problem with the statement is that disobeying unjust laws often has the opposite affect of what was intended or hoped for. 
Most anyone would argue, for instance, that our federal system of income taxation is unfair in one respect or another. 
Yet the end result of widespread disobedience, in this case tax evasion, is to perpetuate the system. 
Free-riders only compel the government to maintain tax rates at high levels in order to ensure adequate revenue for the various programs in its budget.

Yet another fundamental problem with the statement is that by justifying a violation of one
Sort of law we find ourselves on a slippery slope toward sanctioning all types of illegal behavior, including egregious criminal conduct. 
Returning to the abortion example mentioned above, a person strongly opposed to the freedom-of-choice position might maintain that the illegal blocking of access to an abortion clinic amounts to justifiable disobedience. 
However, it is a precariously short leap from this sort of civil disobedience to physical confrontations with clinic workers, then to the infliction of property damage, then to the bombing of the clinic and potential murder.

In sum, because the inherent function of our laws is to balance competing interests,
Reasonable people with different priorities will always disagree about the fairness of specific laws. 
Accordingly, radical action such as resistance or disobedience is rarely justified merely by one's subjective viewpoint or personal interests.
And in any event, disobedience is never justifiable when the legal rights or safety of innocent people are jeopardized as a result.

Whether making things simple requires greater effort and courage than making them bigger and more complex depends on the sort of effort and courage. 
Indisputably, the many complex technological marvels that are part-and-parcel of our Lives today are the result of the extraordinary cumulative efforts of our engineers, entrepreneurs, and others. 
And, such achievements always call for the courage to risk failing in a large way. Yet, humans seem naturally driven to make things bigger and more complex; thus refraining from doing so, or reversing this natural process, takes considerable effort and courage of a different sort, as discussed below.



0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_-1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

The statement brings immediately to mind the ever-growing and increasingly complex digital world. 
Today's high-tech firms seem compelled to boldly go to whatever effort is required to devise increasingly complex products, for the ostensible purpose of staying ahead of their competitors. 
Yet, the sort of effort and courage to which the statement refers is a different one--bred of vision, imagination, and a willingness to forego near term profits for the prospect of making lasting contributions. 
Surely, a number of entrepreneurs and engineers today are mustering that courage, and are making the effort to create far simpler, yet more elegant, technologies and applications, which will truly make our lives simpler in sharp contrast to what computer technology has delivered to us so far.

Lending even more credence to the statement is the so-called "big government" phenomenon. Human societies have a natural tendency to create unwieldy bureaucracies, a fitting example of which is the U.S. tax-law system. 
The Intemal Revenue Code and its accompanying Treasury Regulations have grown so voluminous and complex that many certified accountants and tax attorneys admit that they cannot begin to understand it all. 
Admittedly, this system has grown only through considerable effort on the part of all three branches of the federal government, not to mention the efforts of many special interest groups.
Yet, therein lies the statement's credibility. 
It requires great effort and courage on the part of a legislator to risk alienating special interest groups, thereby risking reelection prospects, by standing on principle for a simpler tax system that is less costly to administer and better serves the interests of most taxpayers.

Adding further credibility to the statement is the tendency of most people to complicate their personal lives--a tendency that seems especially strong in today's age of technology and consumerism. 
The greater our mobility, the greater our number of destinations each day; the more time-saving gadgets we use, the more activities we try to pack into our day; and with readier access to information we try to assimilate more of it each day. 
I am hard-pressed to think of one person who has ever exclaimed to me how much effort and courage it has taken to complicate his or her life in these respects. 
In contrast, a certain self-restraint and courage of conviction are both required to eschew modern conveniences, to simplify one'sdaily schedule, and to establish and adhere to a simple plan for the use of one's time and money.

In sum, whether we are building computer networks, government agencies, or personal lifestyles, great effort and courage are required to make things simple, or to keep them that way. 
Moreover, because humans nature tend to make things big and complex, it arguably requires more effort and courage to move in the opposite direction. 
In the final analysis, making things simple---or keeping them that way--takes a brand of effort born of reflection and restraint rather than sheer exertion, and a courage character and conviction rather than unbridled ambition.


0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_-1,8_0,9_-1,10_1,11_1,12_1,13_1,14_1,15_1

The speaker asserts that wherever a practical, utilitarian need for new buildings arises this need should take precedence over our conflicting interest in preserving historic buildings as a record of our past. 
In my view, however, which interest should take precedence should be determined on a case-by-case basis--and should account not only for practical and historic considerations but also aesthetic ones.

In determining whether to raze an older building, planners should of course consider the community's current and anticipated utilitarian needs. 
For example, if an additional hospital is needed to adequately serve the health-care needs of a fast-growing community, this compelling interest might very well outweigh any interest in preserving a historic building that sits on the proposed site. 
Or if additional parking is needed to ensure the economic survival of a city's downtown district, this interest might take precedence over the historic value of an old structure that stands in the way of a parking structure. 
On the other hand, if the need is mainly for more office space, in some cases an architecturally appropriate add-on or annex to an older building might serve just as well as razing the old building to make way for a new one. 
Of course, an expensive retrofit might not be worthwhile if no amount of retrofitting would meet the need.

Competing with a community's utilitarian needs is an interest preserving the historical record. 
Again, the weight of this interest should be determined on a case-by-case basis. Perhaps an older building uniquely represents a bygone era, or once played a central role in the city's history as a municipal structure. 
Or perhaps the building once served as the home of a founding family or other significant historical figure, or as the location of an important historical event. 
Any of these scenarios might justify saving the building at the expense of the practical needs of the community. 
On the other hand, if several older buildings represent the same historical era just as effectively, or if the building's history is an unremarkable one, then the historic value of the building might pale in comparison to the value of a new structure that meets a compelling practical need.

Also competing with a community's utilitarian needs is the aesthetic and architectural value of the building itself--apart from historical events with which it might be associated. 
A building might be one of only a few that represents a certain architectural style. 
Or it might be especially beautiful, perhaps as a result of the craftsmanship and materials employed in its construction--which might be cost-prohibitive to replicate today. 
Even retrofitting the building to accommodate current needs might undermine its aesthetic as well as historic value, by altering its appearance and architectural integrity. Of course it is difficult to quantify aesthetic value and weigh it against utilitarian considerations. 
Yet planners should strive to account for aesthetic value nonetheless.

In sum, whether to raze an older building in order to construct a new one should never be determined indiscriminately. 
Instead, planners should make such decisions on a case-by-case basis, weighing the community's practical needs against the building's historic and aesthetic value.





0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_-1,8_1,9_1,10_-1,11_-1,12_-1,13_1,14_1,15_1

The speaker makes a threshold claim that students who learn only facts learn very little, then condudes that students should always learn about concepts, ideas, and trends before they memorize facts. 
While I wholeheartedly agree with the threshold claim, the condusion unfairly generalizes about the learning process. 
In fact, following the speaker's advice would actually impede the learning of concepts and ideas, as well as impeding the development of insightful and useful new ones.

Turning first to the speaker's threshold daim, I strongly agree that ifwe learn only facts we learn very little. 
Consider the task of memorizing the periodic table of dements, which any student can memorize without any knowledge of chemistry, or that the table relates to chemistry. Rote memorization of the table amounts to a bit of mental exercise-an opportunity to practice memorization techniques and perhaps learn some new ones. 
Otherwise, the student has learned very little about chemical dements, or about anything for that matter.

As for the speaker's ultimate claim, I concede that postponing the memorization of facts until after one leams ideas and concepts holds certain advantages. 
With a conceptual framework already in place a student is better able to understand the meaning of a fact, and to appreciate its significance. 
As a result, the student is more likely to memorize the fact to begin with, and less likely to forget it as time passes.
Moreover, in my observation students whose first goal is to memorize facts tend to stop there--for whatever reason. 
It seems that by focusing on facts first students risk equating the learning process with the assimilation of trivia; in turn, students risk learning nothing of much use in solving real world problems.

Conceding that students must learn ideas and concepts, as well as facts relating to them, in order to learning anything meaningful, I nevertheless disagree that the former should always precede the latter--for three reasons. 
In the first place, I see know reason why memorizing a fact cannot precede learning about its meaning and significance--as long as the student does not stop at rote memorization. 
Consider once again our hypothetical chemistry student. 
The speaker might advise this student to first learn about the historical trends leading to the discovery of the elements, or to learn about the concepts of altering chemical compounds to achieve certain reactions--before studying the periodic table. 
Having no familiarity with the basic vocabulary of chemistry, which includes the information in the periodic table, this student would come away from the first two lessons bewildered and confused in other words, having learned little.

In the second place, the speaker misunderstands the process by which we learn ideas and concepts, and by which we develop new ones. 
Consider, for example, how economics students learn about the relationship between supply and demand, and the resulting concept of market equilibrium, and of surplus and shortage. 
Learning about the dynamics of supply and demand involves (1) entertaining a theory, and perhaps even formulating a new one, (2) testing hypothetical scenarios against the theory, and (3) examining real-world facts for the purpose of confirming, refuting, modifying, or qualifying the theory. But which step should come first?
The speaker would have us follow steps 1 through 3 in that order. 
Yet, theories, concepts, and ideas rarely materialize out of thin air; they generally emerge from empirical observations--i.e., facts. Thus the speaker's  notion about how we should learn concepts and ideas gets the learning process backwards.

In the third place, strict adherence to the speaker's advice would surely lead to ill-conceived ideas, concepts, and theories. 
Why? An idea or concept conjured up without the benefit of data amounts to little more than the conjurer's hopes and desires. 
Accordingly, conjurers will tend to seek out facts that support their prejudices and opinions, and overlook or avoid facts that refute them. 
One telling example involves theories about the center of the universe. 
Understandably, we ego-driven humans would prefer that the universe revolve around us. 
Early theories presumed so for this reason, and facts that ran contrary to this ego-driven theory were ignored, while observers of these facts were scorned and even vilified. 
In short, students who strictly follow the speaker's prescription are unlikely to contribute significantly to the advancement of knowledge.

To sum up, in a vacuum facts are meaningless, and only by filling that vacuum with ideas
and concepts can students learn, by gaining useful perspectives and insights about facts. Yet, since facts are the very stuff from which ideas, concepts, and trends spring, without some facts students cannot learn much of anything.
In the final analysis, then, students should learn facts right along with concepts, ideas, and trends.



0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_1,9_1,10_-1,11_1,12_1,13_1,14_1,15_1


The speaker asserts that rather than merely highlighting certain sensational events the media should provide complete coverage of more important events.
While the speaker's assertion has merit from a normative standpoint, in the final analysis I find this assertion indefensible.

Upon first impression the speaker's claim seems quite compelling, for two reasons. 
First, without the benefit of a complete, unfiltered, and balanced account of current events, it is impossible to develop an informed and intelligent opinion about important social and political issues and, in turn, to contribute meaningfully to our democratic society, which relies on broad participation in an ongoing debate about such issues to steer a proper course. 
The end result of our being a largely uninformed people is that we relegate the most important decisions to a handful of legislators, jurists, and executives who may or may not know what is best for us.

Second, by focusing on the "sensational"--by which I take the speaker to mean comparatively shocking, entertaining, and titillating events which easily catch one's attention-the media appeal to our emotions and baser instincts, rather than to our intellect and reason. 
Any observant person could list many examples aptly illustrating the trend in this direction--from trashy talk shows and local news broadcasts to The National Enquixer and People Magazine. 
This trend dearly serves to undermine a society's collective sensibilities and renders a society's members more vulnerable to demagoguery; thus we should all abhor and resist the trend.

However, for several reasons I find the media's current trend toward highlights and the sensational to be justifiable. 
First, the world is becoming an increasingly eventful place; thus with each passing year it becomes a more onerous task for the media to attempt full news coverage. 
Second, we are becoming an increasingly busy society. 
The average U.S. worker spends nearly 60 hours per week at work now; and in most families both spouses work. 
Compare this startlingly busy pace to the pace a generation ago, when one bread-winner worked just over 40 hours per week. 
We have far less time today for news, so highlights must suffice. 
Third, the media does in fact provide full coverage of important events; anyone can find such coverage beyond their newspaper's front page, on daily PBS news programs, and on the Internet. 
I would wholeheartedly agree with the speaker if the sensational highlights were all the media were willing or permitted to provide; this scenario would be tantamount to thought control on a mass scale and would serve to undermine our free society. 
However, I am aware of no evidence of any trend in this direction. 
To the contrary, in my observation the media are informing us more fully than ever before; we just need to seek out that information.

On balance, then, the speaker's claim is not defensible.
In the final analysis the media serves its proper function by merely providing what we in a free society demand. 
Thus any argument about how the media should or should not behave--regardless of its merits from a normative standpoint begs the question.




0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_-1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1


This statement is fundamentally correct; public figures should indeed expect to lose their privacy.
After all, we are a society of voyeurs wishing to transform our mundane lives; and one way to do so is to live vicariously through the experiences of others whose lives appear more interesting than our own. 
Moreover, the media recognize this societal foible and exploit it at every opportunity. Nevertheless, a more accurate statement would draw a distinction between political figures and other public figures; the former have even less reason than the latter to expect to be left alone, for the reason that their duty as public servants legitimizes public scrutiny of their private lives.

The chief reason why I generally agree with the statement is that, for better or worse,
intense media attention to the lives of public figures raises a presumption in the collective mind of the viewing or reading public that our public figures' lives are far more interesting than our own. 
This presumption is understandable. 
After all, I think most people would agree that given the opportunity for even fleeting fame they would embrace it without hesitation. 
Peering into the private lives of those who have achieved our dreams allows us to live vicariously through those lives.

Another reason why I generally agree with the statement has to do with the forces that
motivate the media.
 For the most part, the media consist of large corporations whose chief objective is to maximize shareholder profits. 
In pursuit of that objective the media are simply giving the public what they demand a voyeuristic look into the private lives of public figures. 
One need look no further than a newsstand, local-television news broadcast, or talk show to find ample evidence that this is so. 
For better or worse, we love to peer at people on public pedestals, and we love to watch them fall off. 
The media know this all too well, and exploit our obsession at every opportunity.

Nevertheless, the statement should be qualified in that a political figure has less reason to expect privacy than other public figures.
Why? The private affairs of public servants become our business when those affairs adversely affect our servants' ability to serve us effectively, or when our servants betray our trust. 
For example, several years ago the chancellor of a university located in my city was expelled from office for misusing university funds to renovate his posh personal residence. 
The scandal became front-page news in the campus newspaper, and prompted a useful system-wide reform. 
Also consider the Clinton sex scandal, which sparked a debate about the powers and duties of legal prosecutors vis4-vis the chief executive.

Also, the court rulings about executive privilege and immunity, and even the impeachment proceedings, all of which resulted from the scandal, might serve as useful legal precedents for the future.

Admittedly, intense public scrutiny of the personal lives of public figures can carry harmful consequences, for the public figure as well as the society. 
For instance, the Clinton scandal resulted in enormous financial costs to taxpayers, and it harmed many individuals caught up in the legal process. 
And for more that a year the scandal served chiefly to distract us from our most pressing national and global problems. 
Yet, until as a society we come to appreciate the potentially harmful effects of our preoccupation with the lives of public figures, they can expect to remain the cynosures of our attention.




0_1,1_1,2_1,3_1,4_-1,5_0,6_-1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_-1,15_1

The speaker contends that technology's primary goal should be to increase our efficiency for the purpose of affording us more leisure time. 
I concede that technology has enhanced our efficiency as we go about our everyday lives. Productivity software helps us plan and coordinate projects; intranets, the Internet, and satellite technology make us more efficient messengers; and technology even helps us prepare our food and access entertainment more efficiently.
Beyond this concession, however, I find the speaker's contention indefensible from both an empirical and a normative standpoint.

The chief reason for my disagreement lies in the empirical proof: with technological advancement comes diminished leisure time. 
In 1960 the average U.S. family included only one breadwinner, who worked just over 40 hours per week. Since then the average work week has increased steadily to nearly 60 hours today; and in most families there are now two breadwinners. 
What explains this decline in leisure despite increasing efficiency that new technologies have brought about? 
I contend that technology itself is the culprit behind the decline. 
We use the additional free time that technology affords us not for leisure but rather for work. As computer technology enables greater and greater office productivity it also raises our employers' expectations--or demands--for production. 
Further technological advances breed still greater efficiency and, in turn, expectations. 
Our spiraling work load is only exacerbated by the competitive business environment in which nearly all of us work today.
Moreover, every technological advance demands our time and attention in order to learn how to use the new technology. 
Time devoted to keeping pace with technology depletes time for leisure activities.

I disagree with the speaker for another reason as well: the suggestion that technology's chief goal should be to facilitate leisure is simply wrongheaded. 
There are far more vital concerns that technology can and should address.
Advances in bio-technology can help cure and prevent diseases; advances in medical technology can allow for safer, less invasire diagnosis and treatment; advances in genetics can help prevent birth defects; advances in engineering and chemistry can improve the structural integrity of our buildings, roads, bridges and vehicles; information technology enables education while communication technology facilitates global participation in the democratic process. 
In short, health, safety, education, and freedom and not leisure are the proper final objectives of technology.
Admittedly, advances in these areas sometimes involve improved efficiency; yet efficiency is merely a means to these more important ends.

In sum, I find indefensible the speaker's suggestion that technology's value lies chiefly in the efficiency and resulting leisure time it can afford us. 
The suggestion runs contrary to the overwhelming evidence that technology diminishes leisure time, and it wrongly places leisure ahead of goals such as health, safety, education, and freedom as technology's ultimate aims.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

I agree with the speaker's broad assertion that money spent on research is generally money well invested.
However, the speaker unnecessarily extends this broad assertion to embrace research whose results are "controversial," while ignoring certain compelling reasons why some types of research might be unjustifiable. 
My points of contention with the speaker involves the fundamental objectives and nature of research, as discussed below.  

I concede that the speaker is on the correct philosophical side of this issue. 
After all, research is the exploration of the unknown for true answers to our questions, and for lasting solutions to our enduring problems. 
Research is also the chief means by which we humans attempt to satisfy our insatiable appetite for knowledge, and our craving to understand ourselves and the world around us. 
Yet, in the very notion of research also lies my first point of contention with the speaker, who illogically presumes that we can know the results of research before we invest in it. 
To the contrary, if research is to be of any value it must explore uncharted and unpredictable territory. 
In fact, query whether research whose benefits are immediate and predictable can break any new ground, or whether it can be considered "research" at all.

While we must invest in research irrespective of whether the results might be controversial, at the same time we should be circumspect about research whose objectives are too vague and whose potential benefits are too speculative. 
After all, expensive research always carries significant opportunity costs--in terms of how the money might be spent toward addressing society's more immediate problems that do not require research. 
One apt illustration of this point involves the so-called "Star Wars" defense initiative, championed by the Reagan administration during the 1980s. 
In retrospect, this initiative was ill-conceived and largely a waste of taxpayer dollars; and few would dispute that the exorbitant amount of money devoted to the initiative could have gone a long way toward addressing pressing social problems of the day--by establishing after-school programs for delinquent latchkey kids, by enhancing AIDS awareness and education, and so forth.
As it turns out, at the end of the Star Wars debacle we were left with rampant gang violence, an AIDS epidemic, and an unprecedented federal budget deficit.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1

Has creating an image become more important in our society than the reality or truth behind the image?
I agree that image has become a more central concern, at least where short-term business or political success is at stake.
Nevertheless, I think that in the longer term image ultimately yields to substance and fact.

The important role of image is particularly evident in the business world. 
Consider, for example, today's automobile industry. 
American cars are becoming essentially identical to competing Japanese cars in nearly every mechanical and structural respect, as well as in price. 
Thus to compete effectively auto companies must now differentiate their products largely through image advertising, by conjuring up certain illusory benefits--such as machismo, status ,sensibility, or fun. 
The increasing focus on image is also evident in the book-publishing business. 
Publishers are relying more and more on the power of their brands rather than the content of their books. 
Today mass-market books are supplanted within a year with products that are essential the same---except with fresh faces, rifles, and other promotional angles.
I find quite telling the fact that today more and more book publishers are being acquired by large media companies. 
And the increasing importance of image is especially evident in the music industry, where originality, artistic interpretation, and technical proficiency have yielded almost entirely to sex appeal.

The growing significance of image is also evident in the political realm, particularly when it comes to presidential politics.
Admittedly, by its very nature politicking has always emphasized rhetoric and appearances above substance and fact. 
Yet since the invention of the camera presidential politicians have become increasingly concerned about their image. 
For example, Teddy Roosevelt was very careful never to be photographed wearing a tennis outfit, for fear that such photographs would serve to undermine his rough-rider image that won him his only term in office. 
With the advent of television, image became even more central in presidential politics. 
After all, it was television that elected J.F.K. over Nixon. And our only two-term presidents in the television age were elected based largely on their image.
Query whether Presidents Lincoln, Taft, or even F.D.R. would be elected today if pitted against the handsome leading man Reagan, or the suave and poliricaUy correct Clinton. After all, Lincoln was homely, Taft was obese, and F.D.R. was crippled.

In the long term, however, the significance of image wanes considerably.
The image of the Marlboro man ultimately gave way to the truth about the health hazards of cigarette smoking. 
Popular musical acts with nothing truly innovative to offer musically eventually disappear from the music scene. 
And anyone who frequents yard sales knows that today's best-selling books often become tomorrow's pulp.
Even in politics, I think history has a knack for peeling away image to focus on real accomplishments. 
I think history will remember Teddy Roosevelt, for example, primarily for building the Panama Canal and for establishing our National Park System--and not for his rough-and-ready wardrobe.

In the final analysis, it seems that in every endeavor where success depends to some degree on persuasion, marketing, or salesmanship, image has indeed become the central concern of those who seek to persuade. 
And as our lives become busier, our attention spans briefer, and our choices among products and services greater, I expect this trend to continue unabated for better or worse.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

I agree with the statement insofar as our heroes tend to be ordinary people like us.
However, I strongly disagree with the further assertion that people become heroes simply by being "in the right place at the right time." 
If we look around at the sorts of people we choose as our heroes, we real/ze that heroism has far less to do with circumstance than with how a hero responds to it.

I concede that heroes are generally ordinary people.
In my observation we choose as our heroes people with whom we strongly identify--people who are very much like us.
In fact many of us call a parent, grandparent, or older sibling our hero. 
Why? My intuition is that the more a person shares in common with us----m terms of experience, heritage, disposition, motives, and even physical attributes-----~e more accessible that person's heroic traits are to us, and the stronger their attraction as a role model. 
And few would dispute that we share more in common with immediately family than with anyone else.

However, the statement's further suggestion that people become heroes merely as a result of circumstances not of their own choosing is simply wrongheaded. 
Admittedly, circumstance often serves as a catalyst for heroism. 
After all, without wars there would be no war heroes. 
Yet this does not mean that we should lionize every member of the armed forces. I find quite telling the oft-used idiom "heroic effort," which suggests that mere coincidence has little to do with heroism. 
If one examines the sorts of people we select as our heroes, it becomes evident that heroism requires great effort, and that the very nub of heroism lies in the response, not in the circumstance.

Consider the ordinary person who overcomes a personal obstacle through extraordinary effort, fortitude, or faith---thereby inspiring others toward similar accomplishments. 
Sports heroes often fall into this category. 
For example, Lance Armstrong, a Tour de France cycling champion, became a national hero not merely because he won the race but because he overcame a life-threatening illness, against all odds, to do so. 
Of course, widespread notoriety is not a requisite for heroic status. 
Countless individuals with physical and mental disabilities become heroes in their community and among their acquaintances by treating their obstacles as personal challenges--thereby setting inspirational examples. 
Consider the blind law student who inspires others to overcome the same challenge; or the amputee distance runner who serves as a role model for other physically challenged people in her community. 
To assert that individuals such as these become our heroes merely by accident, as the statement seems to suggest, is to completely misunderstand the very stuff of which heroes are made.

Another sort of hero is the ordinary person who attains heroic stature by demonstrating extraordinary courage of conviction--against external oppressive forces. 
Many such heroes are champions of social causes, rising to heroic stature by way of the courage of their convictions; and, it is because we share those convictions--because we recognize these champions as being very much like us----~at they become our heroes. 
Such heroes as India's Mahatma Gandhi, America's Martin Luther King, South Africa's Nelson Mandela, and Poland's Lech Lawesa come immediately to mind. None of these heroes was born into royalty or other privilege; they all came from fairly common, or ordinary, places and experiences. 
Or consider again our military heroes, whose courage and patriotism in battie the statement would serve to completely discredit as merely accidental outcomes of certain soldiers being "m the right place at the right time." I think the preposterousness of such a suggestion is clear enough.

In sum, the statement correctly suggests that heroes are ordinary people like us, and that opportunity, or circumstance, is part of what breeds heroes. 
However, the statement overlooks that serendipity alone does not a hero make. 
Heroism requires that "heroic effort," or better yet a "heroic response," to one's circumstances in life.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

Can a person's greatness be recognized only in retrospect, by those who live after the
person, as the speaker maintains? 
In my view the speaker unfairly generalizes.
In some areas, especially the arts, greatness is often recognizable in its nascent stages. However, in other areas, particularly the physical sciences, greatness must be tested over time before it can be confirmed.
In still other areas, such as business, the incubation period for greatness varies from case to case.

We do not require a rear-view mirror to recognize artistic greatness--whether in music, visual arts, or literature. 
The reason for this is simple: art can be judged at face value. 
There's nothing to be later proved or disproved, affirmed or discredited, or even improved upon or refined by further knowledge or newer technology. 
History is replete with examples of artistic greatness immediately recognized, then later confirmed. 
Through his patronage, the Pope recognized Michelangelo's artistic greatness, while the monarchs of Europe immediately recognized Mozart's greatness by granting him their most generous commissions. 
Mark Twain became a best-selling author and household name even during his lifetime. And the leaders of the modernist school of architecture marveled even as Frank Lloyd Wright was elevating their notions about architecture to new aesthetic heights.

By contrast, in the sciences it is difficult to identify greatness without the benefit of historical perspective. 
Any scientific theory might be disproved tomorrow, thereby demoting the theorist's contribution to the status of historical footnote. 
Or the theory might withstand centuries of rigorous scientific scrutiny. 
In any event, a theory may or may not serve as a springboard for later advances in theoretical science. 
A current example involves the ultimate significance of two opposing theories of physics: wave theory and quantum theory. 
Some theorists now claim that a new so-called "string" theory reconciles the two opposing theories--at least mathematically. 
Yet "strings" have yet to be confirmed empirically. 
Only time will tell whether string theory indeed provides the unifying laws that all matter in the universe obeys. 
In short, the significance of contributions made by theoretical scientists cannot be judged by their contemporaries--only by scientists who follow them.

In the realm of business, in some cases great achievement is recognizable immediately,
while in other cases it is not. 
Consider on the one hand Henry Ford's assembly-line approach to manufacturing affordable cars for the masses. 
Even Ford could not have predicted the impact his innovations would have on the American economy and on the modern world.
On the other hand, by any measure, Microsoft's Bill Gates has made an even greater contribution than Ford; after all, Gates is largely responsible for lifting American technology out of the doldrums during the 1970s to restore America to the status of economic powerhouse and technological leader of the world. 
And this contribution is readily recognizable now--as it is happening. 
Of course, the DOS and Windows operating systems, and even Gates' monopoly, might eventually become historical relics. 
Yet his greatness is already secured.

In sum, the speaker overlooks many great individuals, particularly in the arts and in business, whose achievements were broadly recognized as great even during their own time. 
Nevertheless, other great achievements, especially scientific ones, cannot be confirmed as such without the benefit of historical perspective.




0_1,1_1,2_1,3_1,4_1,5_1,6_-1,7_-1,8_1,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

The speaker contends that people learn just as much from watching television as by   reading books, and therefore that reading books is not as important for learning as it once was. 
I strongly disagree. 
I concede that in a few respects television, including video, can be a more efficient and effective means of learning. 
In most respects, however, these newer media serve as poor substitutes for books when it comes to learning.

Admittedly, television holds certain advantages over books for imparting certain types of knowledge. 
For the purpose of documenting and conveying temporal, spatial events and experiences, film and video generally provide a more accurate and convincing record than a book or other written account. 
For example, it is impossible for anyone, no matter how keen an observer and skilled a journalist, to recount in complete and objective detail such events as a Ballanchine ballet, or the scene at the intersection of Florence and Normandy streets during the 1992 Los Angeles riots. 
Besides, since the world is becoming an increasingly eventful place, with each passing day it becomes a more onerous task for journalists, authors, and book publishers to recount these events, and disseminate them in printed form. 
Producers of televised broadcasts and videos have an inherent advantage in this respect. 
Thus the speaker's claim has some merit when it comes to arts education and to learning about modern and current events.

However, the speaker overlooks several respects m which books are inherently superior to television as a medium for learning. 
Watching television or a video is no indication that any significant learning is taking place; the comparatively passive nature of these media can render them ineffectual in the learning process. 
Also, books are far more portable than television sets. 
Moreover, books do not break, and they do not depend on electricity, batteries, or access to airwaves or cable connections---aU of which may or may not be available in a given place. Finally, the effort required to read actively imparts a certain discipline which serves any person well throughout a lifetime of learning.

The speaker also ignores the decided tendency on the part of owners and managers of television media to ffiter information in order to appeal to the widest viewing audience, and thereby maximize profit. 
And casting the widest possible net seems to involve focusing on the sensational---that is, an appeal to our emotions and baser instincts rather than our intellect and reasonableness. 
The end result is that viewers do not receive complete, unfiltered, and balanced information, and therefore cannot rely on television to develop informed and intelligent opinions about important social and political issues.

Another compelling argument against the speaker's claim has to do with how well books and television serve their respective archival functions. 
Books readily enable readers to review and cross-reference material, while televised broadcasts do not. 
Even the selective review of videotape is far more trouble than it is worth, especially if a printed resource is also available. 
Moreover, the speaker's claim carries the implication that all printed works, fiction and non-fiction alike, not transferred to a medium capable of being televised, are less significance as a result. 
This implication serves to discredit the invaluable contributions of all the philosophers, scientists, poets, and others of the past, upon whose immense shoulders society stands today.

A final argument that books are made no less useful by television has to do with the experience of perusing the stacks in a library, or even a bookstore. 
Switching television channels, or even scanning a video library, simply cannot duplicate this experience. 
Why not? Browsing among books allows for serendipity--unexpectedly coming across an interesting and informative book while searching for something else, or for nothing in particular. 
Moreover, browsing through a library or bookstore is a pleasurable sensory experience for many people--an experience that the speaker would have us forego forever.

In sum, television and video can be more efficient than books as a means of staying abreast of current affairs, and for education in the arts that involve moving imagery. 
However, books facilitate learning in certain ways that television does not and cannot. 
In the final analysis, the optimal approach is to use both media side by side--television to keep us informed and to provide moving imagery, along with books to provide perspective and insight on that information and imagery.




0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

Should academic scholars and researchers be free to pursue whatever avenues of inquiry and research that interest them, no matter how unusual or idiosyncratic, as the speaker asserts? 
Or should they strive instead to focus on those areas that are most likely to benefit society?
 l strongly agree with the speaker, for three reasons.

First of all, who is to decide which areas of academic inquiry are worthwhile? Scholars
cannot be left to decide. 
Given a choice they will pursue their own idiosyncratic areas of interest, and it is highly unlikely that all scholars could reach a fully informed consensus as to what research areas would be most worthwhile. 
Nor can these decisions be left to regulators and legislators, who would bring to bear their own quirky notions about what would be worthwhile, and whose susceptibility to influence renders them untrustworthy in any event.

Secondly, by human nature we are motivated to pursue those activities in which we excel. 
To compel scholars to focus only on certain areas would be to force many to waste their true talents. 
For example, imagine relegating today's preeminent astrophysicist Stephen Hawking to research the effectiveness of affirmative-action legislation in reducing workplace discrimination. 
Admittedly, this example borders on hyperbole. 
Yet the aggregate effect of realistic cases would be to waste the intellectual talents of our world's scholars and researchers. 
Moreover, lacking genuine interest or motivation, a scholar would be unlikely to contribute meaningfully to his or her "assigned" field of study.

Thirdly, it is "idiosyncratic" and "unusual" avenues of inquiry that lead to the greatest
contributions to society.
 Avenues of intellectual and scientific inquiry that break no new ground amount to wasted time, talent, and other resources. 
History is laden with unusual claims by scholars and researchers that turned out stunningly significant--that the sun lies at the center of our universe, that time and space are relative concepts, that matter consists of discrete particles, that humans evolved from other life forms, to name a few. 
One current area of unusual research is terraforming---creating biological life and a habitable atmosphere where none existed before. 
This unusual research area does not immediately address society's pressing social problems. 
Yet in the longer term it might be necessary to colonize other planets in order to ensure the survival of the human race; and after all, what could be a more significant contribution to society than preventing its extinction?

Those who would oppose the speaker's assertion might point out that public universities
should not allow their faculty to indulge their personal intellectual fantasies at taxpayer
expense. 
Yet as long as our universities maintain strict procedures for peer review, pure
quackery cannot persist for very long. 
Other detractors might argue that in certain academic areas, particularly the arts and humanities, research and intellectually inquiry amount to little more than a personal quest for happiness or pleasure. 
This specious argument overlooks the societal benefits afforded by appreciating and cultivating the arts. 
And, earnest study in the humanities affords us wisdom to know what is best for society, and helps us understand and approach societal problems more critically, creatively, and effectively. 
Thus despite the lack of a tangible nexus between certain areas of intellectual inquiry and societal benefit, the nexus is there nonetheless.
In sum, I agree that we should allow academic scholars nearly unfettered freedom of
intellectual inquiry and research within reasonable limits as determined by peer review.
Engaging one's individual talents in one's particular area of fascination is most likely to yield advances, discoveries, and innovations that serve to make the world a better and more interesting place in which to live.






0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

This statement actually consists of two claims: (1) that non-mainstream areas of inquiry are vital in satisfying human needs, and (2) that these areas are therefore vital to society.
 I concede that astrology, fortune-telling, and psychic and paranormal pursuits respond to certain basic human needs. 
However, in my view the potential harm they can inflict on their participants and on society far outweighs their psychological benefits.

Admittedly, these non-mainstream areas of inquiry address certain human needs, which
mainstream science and other areas of intellectual inquiry inherently cannot. 
One such need involves our common experience as humans that we freely make our own choices and decisions in life and therefore carry some responsibility for their consequences. 
Faced with infinite choices, we experience uncertainty, insecurity, and confusion; and we feel remorse, regret, and guilt when in retrospect our choices turn out be poor ones. Understandably, to prevent these bad feelings many people try to shift the burden of making difficult choices and decisions to some nebulous authority outside themselves--by relying on the stars or on a tack of tarot cards for guidance.

Two other such needs have to do with our awareness that we are mortal. 
This awareness brings a certain measure of pain that most people try to relieve by searching for evidence of an afterlife. 
Absent empirical proof that life extends beyond the grave, many people attempt to contact or otherwise connect with the so-called "other side" through paranormal and psychic pursuits. 
Another natural response to the prospect of being separated from our loved ones by death is to search for a deeper connection with others here on Earth and elsewhere, in the present as well as the past. 
This response manifests itself in people's enduring fascination with the paranormal search for extraterrestrial life, with so- called "past life" regression and "channeling," and the like.

While the sorts of pursuits which the speaker lists might be "vital" insofar as they help some people feel better about themselves and about their choices and circumstances, query whether these pursuits are otherwise useful to any individual or society. 
In the first place, because these pursuits are not rooted in reason, they are favorite pastimes of charlatans and others who seek to prey on dupes driven by the aforementioned psychological needs. 
And the dupes have no recourse. 
After all, it is impossible to assess the credibility of a tarot card that tells us how to proceed in life simply because we cannot know where the paths not taken would have led. 
Similarly, we cannot evaluate claims about the afterlife because these claims inherently defy empirical proof--or disproof.

In the second place, without any sure way to evaluate the legitimacy of these avenues of inquiry, participants become vulnerable to self-deception, false hopes, fantastic ideas, and even delusions. 
In turn, so-called "insights" gained from these pursuits can too easily serve as convenient excuses for irrational and unreasonable actions that harm others. 
On a personal level, stubborn adherence to irrational beliefs in the face of reason and empirical evidence can lead to self-righteous arrogance, intolerance, anti-social behavior, and even hatred. 
Moreover, on a societal level these traits have led all too often to holy wars, and to such other atrocities as genocide and mass persecution.

In sum, I concede that the non-mainstream pursuits that the speaker lists are legitimate insofar as they afford many people psychological solace in life. 
However, when such pursuits serve as substitutes for reason and logic, and for honest intellectual inquiry, participants begin to distrust intellect as an impediment to enlightenment.
In doing so, they risk making ill-conceived choices for themselves and unfair judgments about others--a risk that in my view outweighs the psychological rewards of those pursuits.




0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_1,8_1,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

Whether successful leadership requires that a leader follow high ethical and moral
standards is a complex issue--one that is fraught with the problems of defining ethics, morality, and successful leadership in the first place. 
In addressing the issue it is helpful to consider in turn three distinct forms of leadership: business, political, and social-spiritual.

In the business realm, successful leadership is generally defined as that which achieves the goal of profit maximization for a firm's shareholders or other owners. 
Moreover, the prevailing view in Western corporate culture is that by maximizing profits a business leader fulfills his or her highest moral or ethical obligation. 
Many disagree, however, that these two obligations are the same. 
Some detractors claim, for example, that business leaders have a duty to do no intentional harm to their customers or to the society in which they operate--for example, by providing safe products and by implementing pollution control measures. 
Other detractors go further--to impose on business leaders an affirmative obligation to protect consumers, preserve the natural environment, promote education, and otherwise take steps to help alleviate society's problems.

Whether our most successful business leaders are the ones who embrace these additional obligations depends, of course, on one's own definition of business success. In my observation, as business leaders become subject to closer scrutiny by the media and by social activists, business leaders will maximize profits in the long term only by taking reasonable steps to minimize the social and environmental harm their businesses cause. 
This observation also accords with my personal view of a business leader's ethical and moral obligation.

In the political realm the issue is no less complex. 
Definitions of successful political leadership and of ethical or moral leadership are tied up in the means a leader uses to wield his or her power and to obtain that power in the first place.
 One useful approach is to draw a distinction between personal morality and public morality. 
In my observation personal morality is unrelated to effective political leadership. Modern politics is replete with examples of what most people would consider personal ethical failings: the marital indiscretions of President Kennedy, for instance. 
Yet few would disagree that these personal moral choices adversely affected his ability to lead.
In contrast, public morality and successful leadership are more closely connected. Consider the many leaders, such as Stalin and Hitler, whom most people would agree were egregious violators of public morality. 
Ultimately such leaders forfeit their leadership as a result of the immoral means by which they obtained or wielded their power. 
Or consider less egregious examples such as President Nixon, whose contempt for the very legal system that afforded him his leadership led to his forfeiture of it. 
It seems that in the short term unethical public behavior might serve a political leader's interest in preserving his or her power; yet in the long term such behavior invariably results in that leader's down- fall that is, in failure. 

One must also consider a third type of leadership: social-spiritual. 
Consider notable figures such as Gandhi and Martin Luther King, whom few would disagree were eminently successful in leading others to practice the high ethical and moral standards which they advocated. 
However, I would be hard-pressed to name one successful social or spiritual leader whose leadership was predicated on the advocacy of patently unethical or immoral behavior. 
The reason for this is simple: high standards for one's own public morality are prerequisites for successful social-spiritual leadership.

In sum, history informs us that effective political and social-spiritual leadership requires
adherence to high standards of public morality. 
However, when it comes to business leadership the relationship is less clear; successful business leaders must strike a balance between achieving profit maximization and fulfilling their broader obligation to the society, which comes with the burden of such leadership.




0_1,1_1,2_1,3_1,4_-1,5_0,6_1,7_-1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

Which is a better way to prepare young people for leadership: developing in them a spirit of competitiveness or one of cooperation? 
The speaker favors the latter approach, even though some leaders attribute their success to their keenly developed competitive spirit. 
I tend to agree with the speaker, for reasons having to do with our increasingly global society, and with the true keys to effective leadership.

The chief reason why we should stress cooperation in nurturing young people today is that, as tomorrow's leaders, they will face pressing societal problems that simply cannot be solved apart from cooperative international efforts.
 For example, all nations will need to cooperate in an effort to disarm themselves of weapons of mass destruction; to reduce harmful emissions which destroy ozone and warm the Earth to dangerous levels; to reduce consumption of the Earth's finite natural resources; and to cure and prevent diseases before they become global epidemics. 
Otherwise, we all risk self-destruction. In short, global peace, economic stability, and survival of the species provide powerful reasons for developing educational paradigms that stress cooperation over competition.

A second compelling reason for instilling in young people a sense of cooperation over competition is that effective leadership depends less on the latter than the former.
A leader should show that he or she values the input of subordinates--for example, by involving them in decisions about matters in which they have a direct stake. 
Otherwise, subordinates might grow to resent their leader, and become unwilling to devote themselves wholeheartedly to the leader's mission. 
In extreme cases they might even sabotage that mission, or even take their useful ideas to competitors. 
And after all, without other people worth leading a person cannot be a leader let alone an effective one.

A third reason why instilling a sense of cooperation is to be preferred over instilling a sense of competition is that the latter serves to narrow a leader's focus on thwarting the efforts of competitors. 
With such tunnel vision it is difficult to develop other, more creative means of attaining organizational objectives. 
Moreover, such means often involve synergistic solutions that call for alliances, partnerships, and other cooperative efforts with would-be competitors.

Those who would oppose the speaker might point out that a thriving economy depends on  freely competitive business environment, which ensures that consumers obtain high-quality goods and services at low prices. 
Thus key leadership positions, especially in business, inherently call for a certain tenacity and competitive spirit. 
And, a competitive spirit seems especially critical in today's hyper-competitive technology-driven economy, where any leader f~iling to keep pace with ever-changing business and technological paradigms soon fails by the wayside. 
However, a leader's effectiveness as a competitor is not necessarily inconsistent with his or her ability to cooperate with subordinates or with competitors, as noted above.

In sum, ifwe were to take the speaker's advice too far we would risk becoming a world without leaders, who are bred of a competitive spirit. 
We would also risk the key benefits of a free-market economy. 
Nevertheless, on balance I agree that it is more important to instill in young people a sense of cooperation than one of competition. 
The speaker's preference  properly reflects the growing role of cooperative alliances and efforts in solving the world's most pressing problems. 
After all, in a world in which our very survival as a species depends on cooperation, the spirit of even healthy competition, no matter how healthy, is of little value to any of us.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_-1,11_-1,12_1,13_1,14_1,15_1

The speaker asserts that society should place more emphasis on intellect and cognition. 
While the speaker might overlook the benefits of nurturing certain emotions and feelings, on  balance I agree that it is by way of our heads rather than our hearts that we can best ensure the well-being of our society.

I concede that undue emphasis on cultivating the intellect at the expense of healthy emotions can harm an individual psychologically. 
Undue suppression of legitimate and healthy desires and emotions can result in depression, dysfunction, and even physical illness.
In fact, the intellect can mask such problems, thereby exacerbating them. 
To the extent they occur on a mass scale these problems become societal ones--lowering our economic productivity, burdening our health-care and social-welfare systems, and so forth. 
I also concede that by encouraging and cultivating certain positive emotions and feelings--such as compassion and empathy--society dearly stands to benefit.

In many other respects, however, emphasizing emotions and de-emphasizing intellect can carry negative, even dangerous, consequences for any society. 
Our collective sense of fairness, equity, and justice can easily give way to base instincts like hate, greed, and lust for power and domination. 
Thus, on balance any society is better off quelling or at least tempering these sorts of instincts, by nurturing reason, judgment, tolerance, fairness, and understanding--all of which are products of the intellect.

The empirical evidence supporting this position is overwhelming; yet one need look no further than a television set. 
Most of us have been witness to the current trend in trashy talk shows, which eschew anything approaching intellectual discourse in favor of pan &ring to our baser urges and instincts like jealousy, lust and hate. 
Episodes often devolve into anti-social, sometimes violent, behavior on the part of participants and observers alike. 
And any ostensible "lessons learned" from such shows hardly justify the antisocial outbursts that the producers and audiences of these shows hope for.

The dangers of a de-emphasis on intellect are all too evident in contemporary America.
The incidence of hate crimes is increasing at a starting rate; gang warfare is at an all-time high; the level of distrust between African Americans and white America seems to be growing. Moreover, taken to an extreme and on a mass scale, appeal to the emotions rather than the intellect has resulted in humanity's most horrific atrocities, like the Jewish holocaust, as well as in nearly every holy war ever waged throughout history. 
Indeed, suppressing reason is how demagogues and despots gain and hold their power over their citizen-victims.
 In contrast, reason and better judgment are effective deterrents to despotism, demagoguery, and especially to war.

Those opposed to the speaker's position might argue that stressing cognition and intellect at the expense of emotion and feeling would have a chilling effect on artistic creativity, which would work a harm to the society. 
However, even in the arts students must learn theories and techniques, which they then apply to their craft whether it be music performance, dance, or acting. 
And creative writing requires the cognitive ability to understand how language is used and how to best communicate ideas. 
Besides, creative ability is itsdf partly a function of intellect; that is, creative expression is a marriage between cognitive ability and the expression of feelings and emotions.

In sum, emotions and feelings can serve as important catalysts for compassion and for creativity. 
Yet behaviors that are most harmful to any society are also born of emotions and instincts, which the intellect can serve to override. 
The inescapable conclusion, then, is that the speaker is fundamentally correct.





0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

The speaker claims that significant historical events and trends are made possible by groups of people rather than individuals, and that the study of history should emphasize the former instead of the latter. 
I tend to disagree with both aspects of this claim. 
To begin with, learning about key historical figures inspires us to achieve great things ourselves--far more so than learning about the contributions of groups of people. 
Moreover, history informs us that it is almost always a key individual who provide the necessary impetus for what otherwise might be a group effort, as discussed below.

Admittedly, at times distinct groups of people have played a more pivotal role than key individuals in important historical developments. 
For example, history and art appreciation don courses that study the Middle Ages tend to focus on the artistic achievements of particular artists such as Fra Angelico, a Benedictine monk of that period.
However, Western civilization owes its very existence not to a few famous painters but rather to a group of Benedictine nuns of that period. 
Just prior to and during the decline of the Roman Empire, many women fled to join Benedictine monasteries, bringing with them substantial dowries which they used to acquire artifacts, art works, and manuscripts. 
As a result, their monasteries became centers for the preservation of Western culture and knowledge which would otherwise have been lost forever with the fall of the Roman Empire.

However, equally influential was Johannes Gutenberg, whose invention of the printing press several centuries later rendered Western knowledge and culture accessible to every class of people throughout the known world. 
Admittedly, Gutenberg was not single handedly responsible for the outcomes of his invention. Without the support of paper manufacturers, publishers, and distributors, and without a sufficient demand for printed books, Gutenberg would never have become one of"the famous few." 
However, I think any historian would agree that studying the groups of people who rode the wave of Gutenberg's invention is secondary in understanding history to learning about the root historical cause of that wave. 
Generally speaking, then, undue attention to the efforts and contributions of various groups tends to obscure the cause-and-effect relationships with which the study of history is chiefly concerned.

Gutenberg is just one example of an historical pattern in which it is individuals who have been ultimately responsible for the most significant developments in human history. 
Profound scientific inventions and discoveries of the past are nearly all attributable not to forgettable groups of people but to certain key individuals--for example, Copernicus, Newton, Edison, Einstein, Curie, and of course Gutenberg. 
Moreover, when it comes to seminal sociopolitical events, the speaker's claim finds even less support from the historical record. 
Admittedly, sweeping social changes and political reforms require the participation of large groups of people. 
However, I would be hard-pressed to identify any watershed sociopolitical event attributable to a leaderless group. 
History informs us that groups rally only when incited and inspired by key individuals.

The speaker might claim that important long-term sociological trends are often instigated not by key individuals but rather by the masses. 
I concede that gradual shifts in demography, in cultural traditions and mores, and in societal attitudes and values can carry just as significant an historical impact as the words and deeds of "the famous few." 
Yet, it seems that key individuals almost invariably provide the initial spark for those trends. 
For instance, prevailing attitudes about sexual morality stem from the ideas of key religious leaders; and a culture's prevailing values concerning human life are often rooted in the policies and prejudices of political leaders. 
The speaker might also point out that history's greatest architectural and engineering feats--such as the Taj Mahal and the Great W~--- came about only thm~h the efforts of large groups of workers. 
A~, however, it was the famous few--monarchs in these cases whose whims and egos were the driving force behind these accomplishments.

To sum up, with few historical exceptions, history is shaped by key individuals, not by nameless, faceless groups. 
It is the famous few that provide visions of the future, visions which groups then bring to fruition. 
Perhaps the speaker's claim will have more merit at the close of the next millennium since politics and science are being conducted increasingly by consortiums and committees. 
Yet, today it behooves us to continue drawing inspiration from "the famous few," and to continue understanding history chiefly in terms of their influence.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1

Do imaginative works hold more lasting significance than factual accounts, for the reasons the speaker cites? 
To some extent the speaker overstates fiction's comparative significance. 
On balance, however, I tend to agree with the speaker. By recounting various dimensions of the human experience, a fictional work can add meaning to and appreciation of the times in which the work is set. 
Even where a fictional work amounts to pure fantasy, with no historical context, it can still hold more lasting significance than a factual account. 
Examples from literature and film serve to illustrate these points.

I concede that most fictional works rely on historical settings for plot, thematic, and character development. 
informing us about underlying political, economic, and social conditions, factual accounts provide a frame of reference needed to understand and appreciate imaginative works. 
Fact is the basis for fiction, and fiction is no substitute for fact. 
I would also concede that factual accounts are more "accurate" than fictional ones--insofar as they are more objective. 
But this does not mean that factual accounts provide a "more meaningful picture of the human experience." 
To the contrary, only imaginative works can bring an historical period alive by way of creative tools such as imagery and point of view. 
And, only imaginative works can provide meaning to historical events--through the use of devices such as symbolism and metaphor.

Several examples from literature serve to illustrate this point. 
Twain's novels afford us a sense of how 19th-Century Missouri would have appeared through the eyes of 10-year old boys. 
Melville's "Billy Budd" gives the reader certain insights into what travel on the high seas might have been like in earlier centuries, through the eyes of a crewman. 
And the epic poems "Beowulf" and "Sir Gawain and the Green Knight" provide glimpses of the relationships between warriors and their kings in medieval times. 
Bare facts about these historical eras are easily forgettable, whereas creative stories and portrayals such as the ones mentioned above can be quite memorable indeed. 
In other words, what truly lasts are our impressions of what life must have been like in certain places, at certain times, and under certain conditions. 
Only imaginative works can provide such lasting impressions.

Examples of important films underscore the point that creative accounts of the human experience hold more lasting significance than bare factual accounts. 
Consider four of our most memorable and influential films: Citizen Kane, Schindkr5 LaSt, The Wizard of O~ and Star Wars. 
Did Welles' fictional portrayal of publisher William Randolph Hearst or Spielberg's fictional portrayal of a Jewish sympathizer during the holocaust provide a more "meaningful picture of human experience" than a history textbook? 
Did these accounts help give "shape and focus" to reality more so than newsreels alone could? If so, will these works hold more "lasting significance" than bare factual accounts of the same persons and events?
 I think anyone who has seen these films would answer all three questions affirmatively. Or consider The Wizard of O~ and Star Wars. 
Both films, and the novels from which they were adapted, are pure fantasy. 
Yet both teem with symbolism and metaphor relating to life's journey, the human spirit, and our hopes, dreams and ambitions--in short, the human experience. 
Therein lies the reason for their lasting significance.

In sum, without prior factual accounts fictional works set in historical periods lose much of their meaning. 
Yet only through the exercise of artistic license can we convey human experience in all its dimensions, and thereby fully understand and appreciate life in other times and places. 
And it is human experience, and not bare facts and figures, that endures in our minds and souls.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1

Whether college faculty should also work outside academia, in professional work related to their academic fields, depends primarily on the specific academic area. 
With respect to fields in which outside work is appropriate, I strongly agree with the statement; students and faculty all stand to gain in a variety of respects when professor complements academic duties with real-world experience.

As a threshold matter, the statement requires qualification in two respects. 
First, in certain academic areas there is no profession to speak of outside academia. 
This is especially true in the humanities; after all, what work outside academia is there for professors of literature or philosophy? 
Secondly, the statement fails to consider that in certain other academic areas a professor's academic duties typically involve practical work of the sort that occurs outside academia. 
This is especially true in the fine and performing arts, where faculty actively engage in the craft by demonstrating techniques and styles for their students.

Aside from these two qualifications, I strongly agree that it is worthwhile for college faculty to work outside academia in professional positions related to their field. 
There are three dear benefits of doing so. 
First, in my experience as a student, faculty who are actively engaged in their fields come to class with fresh insights and a contagious excitement about the subject at hand. Moreover, they bring to their students practical, real-world examples of the principles and theories discussed in textbooks, thereby sparking interest, and even motivating some students to pursue the field as a career.

Secondly, by keeping abreast with the changing demands of work as a professional,
professors can help students who are serious about pursuing a career in that field to make more informed career decisions. 
The professor with field experience is better able to impart useful, up-to-date information about what work in the field entails, and even about the current job market. 
After all, college career-planning staff are neither equipped nor sufficiently experienced to provide such specific advice to students.

 A third benefit has to do with faculty research and publication in their areas of specialty.
Experience in the field can help a professor ferret out cutting-edge and controversial
issues--which might be appropriate subjects for research and publication. 
Moreover, practical experience can boost a professor's credibility as an expert in the field. 
For example, each year a certain sociology professor at my college combined teaching with undercover work investigating various cults. 
Not only did the students benefit from the many interesting stories this professor had to tell about his experiences, the professor's publications about cults catapulted him to international prominence as an expert on the subject, and justifiably so. 
In sum, aside from certain academic areas in which outside work is either unavailable or unnecessary, students and faculty alike stand everything to gain when faculty enrich their careers by interspersing field work with academic work.




0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

Does recognizing the limits of our knowledge and understanding serve us equally well as acquiring new facts and information, as the speaker asserts? 
While our everyday experience might lend credence to this assertion, further reflection reveals its fundamental inconsistence with our Western view of how we acquire knowledge. 
Nevertheless, a careful and thoughtful definition of knowledge can serve to reconcile the two.

On the one hand, the speaker's assertion accords with the everyday experience of working professionals. 
For example, the sort of"book'I knowledge that medical, law, and business students acquire, no matter how extensive, is of little use unless these students also learn to accept the uncertainties and risks inherent in professional practice and in the business world. 
Any successful doctor, lawyer, or entrepreneur would undoubtedly agree that new precedents and challenges in their fields compel them to acknowledge the limitations of their knowledge, and that learning to accommodate these limitations is just as important in their professional success as knowledge itself.

Moreover, the additional knowledge we gain by collecting more information often diminishes-sometimes to the point where marginal gains turn to marginal losses. 
Consider, for instance, the collection of financial-investment information. 
No amount of knowledge can eliminate the uncertainty and risk inherent in financial investing. Also, information overload can result in confusion, which in turn can diminish one's ability to assimilate information and apply it usefully. 
Thus, by recognizing the limits of their knowledge, and by accounting for those limits when making decisions, investment advisors can more effectively serve their clients.

On the other hand, the speaker's assertion seems self-contradictory, for how can we know the limits of our knowledge until we've thoroughly tested those limits through exhaustive empirical observation--that is, by acquiring facts and information. 
For example, it would be tempting to concede that we can never understand the basic forces that govern all matter in the universe. 
Yet due to increasingly precise and extensive fact-finding efforts of scientists, we might now be within striking distance of understanding the key laws by which all physical matter behaves. 
Put another way, the speaker's assertion flies in the face of the scientific method, whose fundamental tenet is that we humans can truly know only that which we observe. 
Thus Francis Bacon, who first formulated the method, might assert that the speaker is fundamentally incorrect.

How can we reconcile our experience in everyday endeavors with the basic assumption underlying the scientific method? 
Perhaps the answer lies in a distinction between two types of knowledge--one which amounts to a mere collection of observations (i.e., facts and information), the other which is deeper and includes a realization of principles and truths underlying those observations. 
At this deeper level "knowledge" equals "under-standing": how we interpret, make sense of, and find meaning in the information we collect by way of observation.

In the final analysis, evaluating the speaker's assertion requires that we define "knowledge,'' which in turn requires that we address complex epistemological issues best left to philosophers and theologians. 
Yet perhaps this is the speaker's point: that we can never truly know either ourselves or the world, and that by recognizing this limitation we set ourselves free to accomplish what no amount of mere information could ever permit.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_1,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

I fundamentally agree with the speaker's first contention, for unless we embrace the concept of "individual responsibility" our notions of moral accountability and human equality, both crucial to the survival of any democratic society, will whither. 
However, I strongly disagree with the second contention--that our individual actions are determined largely by external forces. 
Although this claim is not entirely without support, it runs contrary to common sense and everyday human experience.

The primary reason that individual responsibility is a necessary fiction is that a society where individuals are not held accountable for their actions and choices is a lawless one, devoid of any order whatsoever. 
Admittedly, under some circumstances a society of laws should carve out exceptions to the rule of individual responsibility--for example, for the hopeless psychotic who has no control over his or her thoughts or actions. 
Yet to extend forgiveness much further would be to endanger the social order upon which any civil and democratic society depends.

A correlative argument for individual responsibility involves the fact that lawless, or anarchist, states give way to despotic rule by strong individuals who seize power. 
History informs us that monarchs and dictators often justify their authority by claiming that they are preordained to assume it--and that as a result they are not morally responsible for their oppressive actions. 
Thus, any person abhorring despotism must embrace the concept of individual responsibility.

As for the speaker's second claim, it flies in the face of our everyday experiences in making choices and decisions. 
Although people often claim that life's circumstances have "forced" them to take certain actions, we all have an infinite number of choices; it's just that many of our choices are unappealing, even self-defeating. 
Thus, the complete absence of free WIU would seem to be possible only in the case of severe psychosis, coma, or death.

Admittedly, the speaker's second contention finds support from "strict determinist" philosophers, who maintain that every event, including human actions and choices, is physically necessary, given the laws of nature. 
Recent advances in molecular biology and genetics lend some credence to this position, by suggesting that these determining physical forces include our own individual genetic makeup. But, the notion of scientific determinism opens the door for genetic engineering, which might threaten equality in socioeconomic opportunity, and even precipitate the development of a "master race." 
Besides, since neither free will nor determinism has been proven to be the correct position, the former is to be preferred by any humanist and in any democratic society.

In sum, without the notion of individual responsibility a civilized, democratic society would soon devolve into an anarchist state, vulnerable to despotic rule. 
Yet, this notion is more than a mere fiction. T
he idea that our actions spring primarily from our free will accords with common sense and everyday experience. 
I concede that science might eventually vindicate the speaker and show that our actions are largely determined by forces beyond our conscious control. 
Until that time, however, I'll trust my intuition that we humans should be, and in fact are, responsible for our own choices and actions.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1

I fundamentally agree with the proposition that students must take courses outside their major field of study to become "truly educated." 
A contrary position would reflect a too narrow view of higher education and its proper objectives. 
Nevertheless, I would caution that extending the proposition too far might risk undermining those objectives.

The primary reason why I agree with the proposition is that "me" education amounts to far more than gaining the knowledge and ability to excel in one's major course of study and in one's professional career. 
True education also facilitates an understanding of one- self, and tolerance and respect for the viewpoints of others. 
Courses in psychology, sociology, and anthropology all serve these ends. "True" education also provides insight and perspective regarding one's place in society and in the physical and metaphysical worlds. 
Courses in political science, philosophy, theology, and even sciences such as astronomy and physics can help a student gain this insight and perspective. 
Finally, no student can be truly educated without having gained an aesthetic appreciation of the world around us--through course work in literature, the fine arts, and the performing arts.

Becoming truly educated also requires sufficient mastery of one academic area to permit a student to contribute meaningfully to society later in life. 
Yet, mastery of any specific area requires some knowledge about a variety of others. 
For example, a political-science student can fully understand that field only by understanding the various psychological, sociological, and historical forces that shape political ideology. 
An anthropologist cannot excel without understanding the social and political events that shape cultures, and without some knowledge of chemistry and geology for performing field work. Even computer engineering is intrinsically tied to other fields, even non-technical ones such as business, communications, and media. 
Nevertheless, the call for a broad educational experience as the path to becoming truly educated comes with one important caveat. 
A student who merely dabbles in a hodgepodge of academic offerings, without special emphasis on any one, becomes a dilettante lacking enough knowledge or experience in any single area to come away with anything valuable to offer. 
Thus in the pursuit of true education students must be careful not to overextend themselves----or risk defeating an important objective of education.

In the final analysis, to become truly educated one must strike a proper balance in one' educational pursuits. 
Certainly, students should strive to excel in the specific requirements of their major course of study. 
However, they should complement those efforts by pursuing course work in a variety of other areas as well. 
By earnestly pursuing a broad education one gains the capacity not only to succeed in a career, but also to find purpose and meaning in that career as well as to understand and appreciate the world and its peoples. 
To gain these capacities is to become "truly educated."





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1

The speaker asserts that because teamwork requires cooperative effort, people are more motivated and therefore more productive working in teams than working individually as competitors. 
My view is that this assertion is true only in some cases. 
If one examines the business world, for example, it becomes clear that which approach is more effective in motivating people and in achieving productivity depends on the specific job.

In some jobs productivity dearly depends on the ability of coworkers to cooperate as members of a team. 
For businesses involved in the production of products through complex processes, all departments and divisions must work in lock-step fashion toward product roll-out. 
Cooperative interaction is even essential in jobs performed in relative isolation and in jobs in which technical knowledge or ability, not the ability to work with others, would seem to be most important. 
For example, scientists, researchers, and even computer programmers must collaborate to establish common goals, coordinate efforts, and meet time lines. Moreover, the kinds of people attracted to these jobs in the first place are likely to be motivated by a sense of common purpose rather than by individual ambition.

In other types of jobs individual competition, tenacity, and ambition are the keys to
productivity. 
For example, a commissioned salesperson's compensation, and sometimes tenure and potential for promotion as well, is based on comparative sales performance of coworkers. 
Working as competitors a firm's individual salespeople maximize productivity-in terms of profit--both for themselves and for their finn. 
Key leadership positions also call, above all, for a certain tenacity and competitive spirit. 
A finn's founding entrepreneur must maintain this spirit in order for the firm to survive, let alone to maximize productivity. 
Moreover, in my observation the kinds of people inclined toward entrepreneurship and sales in the first place are those who are competitive by nature, not those who are motivated primarily by a sense of common purpose.

On balance, however, my view is that cooperation is more crucial for an organization's long-term productivity than individual competition. 
Even in jobs where individual competitiveness is part-and-parcel of the job, the importance of cooperation should not be underestimated. 
Competition among sales people can quickly grow into jealousy, back stabbing, and unethical behavior all of which are counterproductive.
And even the most successful entrepreneurs would no doubt admit that without the cooperative efforts of their subordinates, partners, and colleagues, their personal visions would never become reality.

In sum, individual competitiveness and ambition are essential motivating forces for certain types of jobs, while in other jobs it is a common sense of mission that motivates workers to achieve maximum productivity. 
In the final analysis, however, the overall productivity of almost every organization depends ultimately on the ability of its members to cooperate as a team.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_1,11_1,12_-1,13_1,14_1,15_1

The speaker asserts that the curriculum of colleges and universities should emphasize popular culture--music, media, literature, and so forth rather than literature and art of the past, for the reason that the former is more relevant to students.
 I strongly disagree. 
Although courses in popular culture do play a legitimate role in higher education, formal study of the present culture at the expense of studying past cultures can undermine the function of higher education, and ultimately provide a disservice to students and to society.

Admittedly, course work in popular culture is legitimate and valuable for three reasons. 
First, popular culture is a mirror of society's impulses and values. 
Thus, any serious student of the social sciences, as well as students of media and communications, should take seriously the literature and art of the present. Secondly, in every age and culture some worthwhile art and literature emerges from the mediocrity. 
Few would disagree, for example, that the great modem-jazz pioneers such as Charlie Parker and Thelonius Monk, and more recently Lennon
and McCarmey, and Stevie Wonder, have made just as lasting a contribution to music as some of the great classical musicians of previous centuries. 
Thirdly, knowledge of popular films music, and art enables a person to find common ground to relate to other people. 
This leads to better communication between different subcultures.

Nevertheless, emphasizing the study of popular culture at the expense of studying classical art and literature can carry harmful consequences for students, as well as for society. 
Without the benefit of historical perspective gamed through the earnest study of the art and literature of the past, it is impossible to fully understand, appreciate, and critique literature and art of the present. 
Moreover, by approaching popular culture without any yardstick for quality it is impossible to distinguish mediocre art from worthwhile art. 
Only by studying the classics can an individual develop fair standards for judging popular works. Besides, emphasis on the formal study of popular culture is unnecessary. 
Education in popular culture is readily available outside the classroom---on the Internet, through educational television programming, and through the sorts of everyday conversations and cross-talk that occur at water coolers and in the coffee houses of any college campus.

In sum, while the study of popular literature and art can be worthwhile, it has to be
undertaken in conjunction with an even greater effort to learn about the literature and art of the past. 
In the absence of the latter, our universities will produce a society of people with no cultural perspective, and without any standards for determining what merits our attention and nurtures society.





0_1,1_1,2_1,3_1,4_1,5_0,6_-1,7_1,8_0,9_1,10_-1,11_-1,12_1,13_1,14_1,15_1

I strongly agree with the contention that we often limit our own freedom through our habits and attitudes. 
By limiting our own freedom, we often serve our own interests. 
And as we learn this lesson, we cultivate certain attitudes and habits--particularly in our relationships with others--by which we apply that lesson, and which continue throughout life.

To appreciate that from an early age we ingrain in ourselves habits that serve to constrain.
To our freedom, one need look no further than the neighborhood playground. Even without adult supervision, a group of youngsters at play invariably establish mutually agreed-upon rules of conduct--whether or not a sport or game is involved. Children learn that without any rules for behavior the playground bully usually prevails. 
Thus our habit of making choices that constrain our own freedom stems from our desire to protect our own interests, and it begins at an early age.

This habit of making choices that constrain our own freedom continues into our adult lives. 
As we mature, most of us develop the attitude that monogamous relationships are preferable to polygamous ones--thus our habit of entering into exclusive pair-bonding relationships. 
During our teens we agree to "go steady," then as adults we voluntarily enter into marriage contracts. 
As we enter the working world, we carry these attitudes and habits with us. 
We eagerly engage in exclusive employment relationships---with the attitude that the security of steady income is preferable to the "freedom" of not knowing where our next paycheck will come from. 
Even people who prefer self-employment to job security quickly develop the attitude that the only way to preserve their autonomy is to constrain themselves in terms of their agreements with clients and customers, and especially in terms of how they use their me.

Those who disagree that we tend to restrict our own freedom through our habits and attitudes involving personal and employment relationships might cite the often-heard complaint about life's circumstances leaving one with "no choice." 
One complaining person might feel trapped in a job or a marriage, by their boss or partner.
 Another complainant might blame his or her spendthrift habits on enticing advertisements, the pressure to appear successful, and so forth. 
However, people in situations such as these are not actually at the mercy of others. Instead, they have a significant degree of personal freedom, but simply choose one alternative over others that might be less appealing or even self-defeating. 
For example, almost every person who blames someone else for being trapped in a job is simply choosing to retain a certain measure of financial security. 
The choice to forego this security is always available, although it might carry unpleasant consequences.

That through our attitudes we serve to constrain our own freedom is evident on a societal level as well. 
Just as children at a playground quickly develop the habit of imposing rules and regulations on themselves, as a society we do the same. 
After all, in a democracy our system of laws is an invention of the people. For example, we insist on being bound by restrictions for operadng motor vehides, for buying and selling both real and personal property, and for making public statements about other people. 
Without these restrictions, we would live in continual fear for our physical safety, the security of our property, and our personal reputation and dignity. 
Thus most of the rules and regulations we claim are imposed on us we have
ultimately imposed on ourselves, as a society, in order to protect ourselves.

In the final analysis, in contenting that our habits and attitudes "often" serve to restrict our freedom more than restraints that others place on us do, the statement does not even go far enough. 
Despite our occasional sense that others are restricting our choices, on both an individual and a societal level we are ultimately the ones who, through our attitudes and habits, limit our own freedom.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1

I agree with the speaker insofar as that a practical, pragmatic approach toward our endeavor can help us survive in the short tenn. 
However, idealism is just as crucial if not more so—for long-term success in any endeavor, whether it be in academics, business, or political and social reform.

When it comes to academics, students who we would consider pragmatic tend not to pursue an education for its own sake. 
Instead, they tend to cut whatever corners are needed to optimize their grade average and survive the current academic term. 
But, is this approach the only way to succeed academically? Certainly not. 
Students who eamesdy pursue intellectual paths that truly interest them are more likely to come away with a meaningful and lasting education. 
In fact, a sense of mission about one's area of fascination is strong motivation to participate actively in class and to study earnesdy, both of which contribute to better grades in that area. 
Thus, although the idealist-student might sacrifice a high overall grade average, the depth of knowledge, academic discipline, and sense of purpose the student gains will serve that student well later in life.

In considering the business world it might be more tempting to agree with the speaker; after all, isn't business fundamentally about pragmatism--that is, "getting the job done" and paying attention to the "bottom line"? Emphatically, no. Admittedly, the everyday machinations of business are very much about meeting mundane short-term goals: deadlines for production, sales quotas, profit margins, and so forth. 
Yet underpinning these activities is the vision of the company's chief executive--a vision which might extend far beyond mere profit maximization to the ways in which the frrm can make a lasting and meaningful contribution to the community, to the broader economy, and to the society as a whole.
 Without a dream or vision--that is, without strong idealist leadership--a firm can easily be cast about in the sea of commerce without dear direction, threatening not only the fLrm's bottom line but also its very survival.

Finally, when it comes to the political arena, again at fzrst blush it might appear that pragmatism is the best, if not the only, way to succeed. Most politicians seem driven by their interest in being elected and reelected--that is, in surviving--rather than by any sense of mission, or even obligation to their constituency or country. Diplomatic and legal maneuverings and negotiations often appear intended to meet the practical needs of the parties involved--minimizing costs, preserving options, and so forth. But, it is idealists-not pragmatists--who sway the masses, incite revolutions, and make political ideology reality. Consider idealists such as America's founders, Mahatma Gandhi, or Martin Luther IGng. Had these idealists concerned themselves with short-term survival and immediate needs rather than with their notions of an ideal society, the United States and India might still be British colonies, and African-Americans might still be relegated to the backs of buses.

In short, the statement fails to recognize that idealism--keeping one's eye on an ultimate prize--is the surest path to long-term success in any endeavor.
Meeting one's immediate needs, while arguably necessary for short-term survival, accomplishes lite without a sense of mission, a vision, or a dream for the long term.





0_1,1_1,2_1,3_1,4_1,5_1,6_-1,7_-1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1

The speaker alleges that studying history is valuable only insofar as it is relevant to our daily lives. 
I find this allegation to be specious. It wrongly suggests that history is not otherwise instructive and that its relevance to our everyday lives is limited. 
To the contrary, studying history provides inspiration, innumerable lessons for living, and useful value-clarification and perspective---all of which help us decide how to live our lives.

To begin with, learning about great human achievements of the past provides inspiration. 
For example, a student inspired by the courage and tenacity of history's great explorers might decide as a result to pursue a career in archeology, oceanography, or astronomy. 
This decision can, in turn, profoundly affect that student's everyday life--in school and beyond. 
Even for students not inclined to pursue these sorts of careers, studying historical examples of courage in the face of adversity can provide motivation to face their own personal fears in life. 
In short, learning about grand accomplishments of the past can help us get through the everyday business of living, whatever that business might be, by emboldening us and lifting our spirits.

In addition, mistakes of the past can teach us as a society how to avoid repeating those mistakes. 
For example, history can teach us the inappropriateness of addressing certain social issues, particularly moral ones, on a societal level. 
Attempts to legislate morality invariably fail, as aptly illustrated by the Prohibition experiment in the U.S. during the 1930s. 
Hopefully, as a society we can apply this lesson by adopting a more enlightened legislative approach toward such issues as free speech, criminalization of drug use, criminal justice, and equal rights under the law.

Studying human history can also help us understand and appreciate the mores, values, and ideals of past cultures. 
A heightened awareness of cultural evolution, in turn, helps us formulate informed and reflective values and ideals for ourselves.
 Based on these values and ideals, students can determine their authentic life path as well as how they should allot their time and interact with others on a day-to-day basis

Finally, it might be tempting to imply from the speaker's allegation that studying history has little relevance even for the mundane chores that occupy so much of our time each day, and therefore is of little value. 
However, from history we learn not to take everyday activities and things for granted. 
By understanding the history of money and banking we can transform an otherwise routine trip to the bank into an enlightened experience, or a visit to the grocery store into an homage to the many inventors, scientists, engineers, and entrepreneurs of the past who have made such convenience possible today. 
And, we can fully appreciate our freedom to go about our daily lives largely as we choose only by understanding our political heritage. 
In short, appreciating history can serve to elevate our everyday chores to richer, more interesting, and more enjoyable experiences.
 In sum, the speaker fails to recognize that in all our activities and decisions--from our grandest to our most rote--history can inspire, inform, guide, and nurture. 
In the final analysis, to study history is to gain the capacity to be more human--and I would be hard- pressed to imagine a worthier end.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1

The speaker asserts that a culture perpetuates the ideas it favors while discrediting those It fears primarily through formal education. 
I agree that grade-school, and even high-school education involves cultural indoctrination. Otherwise, I think the speaker misunderstands the role of higher education, and overlooks other means by which a culture achieves these ends.

I agree with the speaker with respect to formal grade-school and even high-school education--which to some extent amount to indoctrination with the values, ideas, and principles of mainstream society. 
In my observation, young students are not taught to question authority, to take issue with what they are taught, or to think critically for themselves. 
Yet, this indoctrination is actually desirable to an extent. 
Sole emphasis on rote learning of facts and figures is entirely appropriate for grade-school children, who have not yet gained the intellectual capacity and real-world experience to move up to higher, more complex levels of thinking. 
Nevertheless, the degree to which our grade schools and high schools emphasize indoctrination should not be overstated. 
After all, cultural mores, values, and biases have little to do with education in the natural sciences, mathe matics, and specific language skills such as reading and writing.

Although the speaker's assertion has some merit when it comes to the education of young people, I find it erroneous when it comes to higher education. 
The mission of our colleges and universities is to afford students cultural perspective and a capacity for understanding opposing viewpoints, and to encourage and nurture the skills of critical analysis and skepticism--not to indoctrinate students with certain ideas while quashing others. 
Admittedly, colleges and universities are bureaucracies and therefore not immune to political influence over what is taught and what is not. 
Thus to some extent a college's curriculum is vulnerable to wealthy and otherwise influential benefactors, trustees, and government agencies who by advancing the prevailing cultural agenda serve to diminish a college's effectiveness in carrying out its true mission. 
Yet, my intuition is that that such influences are minor ones, especially in public university systems.

The speaker's assertion is also problematic in that it ignores two significant other means by which our culture perpetuates ideas it favors and discredits ideas it fears. 
One such means is our system of laws, by which legislators and jurists formulate and then impose so-called "public policy." 
Legislation and judicial decisions carry the weight of law and the threat of punishment for those who deviate from that law. 
As a result, they are highly effective means of forcing on us official notions of what is good for society and for quashing ideas that are deemed threatening to the social fabric, and to the safety and security of the government and the governed. 
A second such means is the mainstream media. By mirroring the culture's prevailing ideas and values, broadcast and print media serve to perpetuate them.
 It is important to distinguish here between mainstream media-such as broadcast television—and alternative media such as documentary films and non-commercial websites, whose typical aims are to call into question the status quo, expose the hypocrisy and unfair bias behind mainstream ideas, and bring to light ideas that the powers-that-be most fear. 
Yet, the influence of alternative media pales in comparison to that of mainstream media.

In sum, the speaker's assertion is not without merit when it comes to the role of grade schools and high schools. 
However, the speaker over-generalizes about what students are taught--especially at colleges and universities. 
Moreover, the speaker's assertion ignores other effective ways in which mainstream culture perpetuates its agenda.






0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_-1,8_0,9_1,10_-1,11_1,12_-1,13_1,14_1,15_1

I strongly agree that the more government proceedings--debates, meeting, and so forth---that are televised, the more society will benefit overall. 
Nevertheless, undue emphasis on this means of informing a constituency has the potential for harm--which any society must take care not to allow.

Access to government proceedings via television carries several significant benefits. The main benefit lies in two useful archival functions of videotaped proceedings. 
First, videotapes are valuable supplements to conventional means of record keeping. 
Although written transcripts and audio tapes might provide an accurate record of what is said, only video tapes can convey the body language and other visual clues that help us understand what people say, whether they are being disingenuous, sarcastic, or sincere. 
Secondly, videotape archives provide a useful catalogue for documentary journalists.

Televised proceedings also provide three other useful functions.
 First, for shut-ins and people who live in remote regions, it might be impracticable, or even impossible, to view government proceedings in person. 
Secondly, with satellite television systems it is possible to witness the governments of other cities, states, and even nations at work. 
This sort of exposure provides the viewer a valuable sense of perspective, an appreciation for other forms of government, and so forth. 
Thirdly, in high schools and universities, television proceedings can be useful curriculum supplements for students of government, public policy, law, and even public speaking.

Nevertheless, televising more and more government proceedings carries certain risks that should not be ignored. 
Watching televised government proceedings is inherently a rather passive experience. 
The viewer cannot voice his or her opinions, objections, or otherwise contribute to what is being viewed.
Watching televised proceedings as a substitute for active participation in the political process can, on a mass scale, undermine the democratic process by way of its chilling effect on participation. 
Undue emphasis on tele government poses the risk that government proceedings will become mere displays, or shows, for the public, intended as public relations ploys and so-called "photo opportunities,'' while the true business of government is moved behind closed doors.

In sum, readier access to the day-day business of a government can only serve to inform and educate. 
Although undue reliance on televised proceedings for information can quell active involvement and serve as a censor for people being televised, I think these are risks worth taking in the interest of disclosure.





0_1,1_-1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_1,11_-1,12_-1,13_1,14_1,15_1

The speaker asserts that the many ads which make consumers want to "be like" the person portrayed in the ad are effective not only in selling products but also in helping consumers feel better about themselves. 
This assertion actually consists of two claims: that this advertising technique is used effectively in selling many products, and that consumers who succumb to technique actually feel better about themselves as a result. 
While I agree with the first claim, I strongly disagree with the second one.

Turning first to the statement's threshold claim, do many ads actually use this technique to sell products in the first place? 
Consider ads like the wildly popular Budweiser commercial featuring talking frogs. There's nothing in that ad to emulate; its purpose is merely to call attention to itself. 
Notwithstanding this type of ad, in my observation the majority of ads provide some sort of model that most consumers in the target market would want to emulate, or "be like." 
While some ads actually portray people who are the opposite of what the viewer would want to "be like," these ads invariably convey the explicit message that to avoid being like the person in the ad the consumer must buy the advertised product. 
As for whether the many, many ads portraying models are effective in selling products, I am not privy to the sort of statistical information required to answer this question with complete certainty. 
However, my intuition is that this technique does help sell products; otherwise, advertisers would not use it so persistently.

Turning next to the statement's ultimate claim that these ads are effective because they help people who buy the advertised products feel better about themselves, I find this claim to be specious. 
Consumers lured by the hope of "being like" the person in an ad might experience some initial measure of satisfaction in the form of an ego boost. 
We have all experienced a certain optimism immediately after acquiring something we've wanted a good feeling that we're one step closer to becoming who we want to be. 
However, in my experience this sense of optimism is ephemeral, invariably giving way to disappointment that the purchase did not
live up to its implicit promise.

One informative example of this false hope involves the dizzying array of diet aids, skin creams, and fitness machines available today. 
The people in ads for these products are youthful, fit, and attractive what we all want to "be like." 
And the ads are effective in selling these products; today's health-and-beauty market feeds a multi-billion dollar industry. 
But the end result for the consumer is an unhealthy preoccupation with physical appearance and youth, which often leads to low self-esteem, eating disorders, injuries from over-exercise, and so forth.
 And these problems are sure signs of consumers who feel worse, not better, about themselves as a result of having relied on the false hope that they will "be like" the model in the ad.

Another informative example involves products that pander to our desire for socioeco-nomic status. 
Ads for luxury cars and upscale dothing typically portray people with lucrative careers living in exclusive neighborhoods. 
Yet, I would wager that no person whose life-style actually resembles these portrayals could honestly claim that purchasing certain consumer products contributed one iota to his or her socioeconomic success. 
The end result for the consumer is envy of others that can afford even more expensive possessions, and ultimately low self-esteem based on feelings of socioeconomic inadequacy.

In sum, while ads portraying people we want to "be like" are undoubtedly effective in selling products, they are equally ineffective in helping consumers feel better about themselves. 
In fact, the result is a sense of false hope, leading ultimately to disappointment and a sense of failure and inadequacy--in other words, feeling worse about ourselves.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_-1,11_1,12_1,13_1,14_1,15_1

Are all historians essentially storytellers, for the reasons that the speaker cites? 
In asserting that we can never know the past directly, the speaker implies that we truly "know" only what we experience first-hand. 
Granting this premise, I agree that it is the proper and necessary role of historians to "construct" history by interpreting evidence. 
Nevertheless, the speaker's characterization of this role as "storytelling" carries certain unfair implications, which should be addressed.

One reason why I agree with the speaker's fundamental claim lies in the distinction between the role of historian and the roles of archivist and journalist. 
By "archivist" I refer generally to any person whose task is to document and preserve evidence of past events. 
And by "journalist" I mean any person whose task is to record, by writing, film, or some other media, factual events as they occur--for the purpose of creating evidence of those events. 
It is not the proper function of either the journalist or the archivist to tell a story. Rather, it is their function to provide evidence to the historian, who then pieces together the evidence to construct history, as the speaker suggests.
 In other words, unless we grant to the historian a license to "construct" history by interpreting evidence, we relegate the historian to the role of mere archivist or journalist.

Another reason why I agree with the speaker's characterization of the historian's proper function is that our understanding of history is richer and fuller as a result. By granting the historian license to interpret evidence--to "construct" history--we allow for differing viewpoints among historians. 
Based on the same essential evidence, two historians might disagree about such things as the contributing causes of a certain event, the extent of influence or impact of one event on subsequent events, the reasons and motives for the words and actions of important persons in history, and so forth. 
The inexorable result of disagreement, debate, and divergent interpretations among historians is a fuller and more incisive understanding of history.

However, we should be careful not to confuse this license to interpret history, which is needed for any historian to contribute meaningfully to our understanding of it, with artistic license. 
The latter should be reserved for dramatists, novelists, and poets. 
It is one thing to attempt to explain historical evidence; it is quite another to invent evidence for the sake of creating a more interesting story or to bolster one's own point of view. 
A recently released biography of Ronald Reagan demonstrates that the line which historians should not cross is a fine one indeed. 
Reagan's biographer invented a fictional character who provided commentary as a witness to key episodes during Reagan's life. 
Many critics charge that the biographer overstepped his bounds as historian; the biographer claims, however, that the accounts in the biography were otherwise entirely factual, and that the fictional narrator was merely a literary device to aid the reader in understanding and appredating the historical Reagan.

In sum, I strongly agree that the historian's proper function is to assemble evidence into plausible constructs of history, and that an element of interpretation and even creativity is properly involved in doing so. 
And if the speaker wishes to call these constructs "storytelling," that's fine. 
This does not mean, however, that historians can or should abandon scholarship for the sake of an interesting story.





0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_-1,8_0,9_1,10_1,11_1,12_1,13_1,14_1,15_1

The speaker asserts that educational systems should place less emphasis on reason and logical thinking and more emphasis on the exploration of emotions. 
While I concede that in certain fields students are well served by nurturing their emotions and feelings, in most academic disciplines it is by cultivating intellect rather than emotions that students master their discipline and, in turn, gain a capacity to contribute to the well-being of society.

I agree with the speaker insofar as undue emphasis on reason and logical thinking can have a chilling effect on the arts. 
After all, artistic ideas and inspiration spring not from logic but from emotions and feelings such as joy, sadness, hope, and love. 
And, the true measure of artistic accomplishment lies not in technical proficiency but rather in a work's impact on the emotions and spirit. 
Nevertheless, even in the arts, students must learn theories and techniques, which they then apply to their craft. 
And, creative writing requires the cognitive ability to understand how language is used and how to communicate ideas. 
Besides, creative ability is itself partly a function of intellect; that is, creative expression is a marriage of one's cognitive abilities and the expression of one's feelings and emotions.

Aside from its utility in the arts, however, the exploration of emotions has little place in educational systems. 
The physical sciences and mathematics are purely products of reason and logic. Even in the so-called "soft" sciences, emotion should play no part. 
Consider, for example, the study of history, political science, or public policy, each of 'which is largely the study of how the concepts of fairness, equity, and justice work themselves out. 
It is tempting to think that students can best understand and learn to apply these concepts by tapping feelings such as compassion, empathy, sympathy, and indignation. 
Yet fairness, equity, and justice have little to do with feelings, and everything to do with reason. 
After all, emotions are subjective things. On the other hand, reason is objective and therefore facilitates communication, consensus, and peaceful compromise.

Indeed, on a systemic scale undue emphasis on the exploration of our emotions can have deleterious societal consequences. 
Emotions invite irrationality in thought and action, the dangers of which are all too evident in contemporary America. 
For example, when it comes to the war on drugs, free speech and religion, abortion issues, and sexual choices, public policy today seems to simply mirror the voters' fears and prejudices.
 Yet common sense dictates that social ills are best solved by identifying cause-and-effect relation-ships---in other words, through critical thinking. 
The proliferation of shouting-match talk shows fueled by irrationality and emotion gone amuck is further evidence that our culture lends too much credence to our emotions and not enough to our minds. 
A culture that sanctions irrationality and unfettered venting of emotion is vulnerable to decline Indeed, exploiting emotions while suppressing reason is how demagogues gain and hold power, and how humanity's most horrific atrocities have come to pass. 
In contrast, reason and better judgment are effective deterrents to incivility, despotism, and war.

In sum, emotions can serve as important catalysts for academic accomplishment in the arts. 
Otherwise, however, students, and ultimately society, are better off by learning to temper their emotions while nurturing judgment, tolerance, fairness, and understanding--all of which are products of reason and critical thinking.




0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_1,9_1,10_1,11_1,12_1,13_1,14_1,15_1

I strongly agree that we define ourselves primarily through our identification with social groups, as the speaker asserts. 
Admittedly, at certain stages of life people often appear to define themselves in other terms. 
Yet, in my view, during these stages the fundamental need to define one's self through association with social groups is merely masked or suspended.

Any developmental psychologist would agree that socialization with other children plays a critical role in any child's understanding and psychological development of self. 
At the day-care center or in the kindergarten class young children quickly learn that they want to play with the same toys at the same time or in the same way as some other children.
 They come to understand generally what they share in common with certain of their peers---m terms of appearance, behavior, likes and dislikes--and what they do not share in common with other peers or with older students and adults. 
In other words, these children begin to recognize that their identity inextricably involves their kinship with certain peers and alienation from other people.

As children progress to the social world of the playground and other after-school venues, their earlier recognition that they relate more closely to some people than to others evolves into a desire to form well-defined social groups, and to set these groups apart from others. 
Girls begin to congregate apart from boys; clubs and cliques are quickly formed--often with exclusive rituals, codes, and rules to further distinguish the group's members from other children. 
This apparent need to be a part of an exclusive group continues through high school, where students identify themselves in their yearbooks by the clubs to which they belonged. 
Even in college, students eagerly join clubs, fraternities, and sororities to establish their identity as members of social groups. 
In my observation children are not taught by adults to behave in these ways; thus this desire to identify oneself with an exclusive social group seems to spring from some innate psychological need to define one's self through one's personal associations.

However, as young adults take on the responsibilities of partnering, parenting, and working, they appear to define themselves less by their social affiliations and more by their marital status, parental status, and occupation. 
The last of these criteria seems particularly important for many adults today. 
When two adults meet for the first time, beyond initial pleasantries the initial question almost invariably is "What do you do for a living?" Yet in my opinion this shift in focus from one's belonging to a social group to one's occupation is not a shift in how we prefer to define ourselves.
 Rather, it is born of economic necessity--we don't have the leisure time or financial independence to concern ourselves with purely social activities. 
I find quite telling the fact that when older people retire from the world of work an interest in identifying with social groups--whether they be bridge clubs, investment clubs, or country clubs--seems to reemerge. 
In short, humans seem possessed by an enduring need to be part of a distinct social group—a need that continues throughout life's journey.

In sum, I agree that people gain and maintain their sense of self primarily through their belonging to distinct social groups. 
Admittedly, there will always be loners who prefer not to belong, for whatever reasons; yet loners are the exception. 
Also, while many working adults might temporarily define themselves in terms of their work for practicality's sake, at bottom we humans are nothing if not social animals.






0_1,1_1,2_1,3_1,4_1,5_0,6_1,7_1,8_0,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

Have technological innovations of the last century failed to bring about true progress for humanity, as the statement contends? 
Although I agree that technology cannot ultimately prevent us from harming one another, the statement fails to account for the significant positive impact that the modem-industrial and computer revolutions have had on the quality of life at least in the developed world.

I agree with the statement insofar as there is no technological solution to the enduring problems of war, poverty, and violence, for the reason that they stem from certain aspects of human nature--such as aggression and greed. 
Although future advances in biochemistry might enable us to "engineer away" those undesirable aspects, in the meantime it is up to our economists, diplomats, social reformers, and jurists--not our scientists and engineers—to mitigate these problems.


Admittedly, many technological developments during the last century have helped reduce human suffering. 
Consider, for instance, technology that enables computers to map Earth's geographical features from outer space. 
This technology allows us to locate lands that can be cultivated for feeding malnourished people in third-world countries. 
And, few would disagree that humanity is the beneficiary of the myriad of 20th-Century innovations in medicine and medical technology--from prostheses and organ transplants to vaccines and lasers.

Yet, for every technological innovation helping to reduce human suffering is another that has served primarily to add to it. 
For example, while some might argue that nuclear weapons serve as invaluable "peace-keepers," this argument flies in the face of the hundreds of thousands of innocent people murdered and maimed by atomic blasts. 
More recently, the increasing use of chemical weapons for human slaughter points out that so called "advances" in biochemistry can amount to net losses for humanity.

Notwithstanding technology's limitations in preventing war, poverty, and violence,
20th-Century technological innovation has enhanced the overall standard of living and comfort level of developed nations. 
The advent of steel production and assembly-line manufacturing created countless jobs, stimulated economic growth, and supplied a plethora of innovative conveniences. 
More recently, computers have helped free up our time by performing repetitive tasks; have aided in the design of safer and more attractive bridges, buildings, and vehicles; and have made possible universal access to information.

Of course, such progress has not come without costs. One harmful byproduct of industrial progress is environmental pollution, and its threat to public health. Another is the alienation of assembly-line workers from their work. 
And, the Internet breeds information overload and steals our time and attention away from family, community, and coworkers. 
Nevertheless, on balance both the modern-industrial and computer revolutions have improved our standard of living and comfort level; and both constitute progress by any measure.

In , enduring problems such as war, poverty, and violence ultimately spring from human nature, which no technological innovation short of genetic engineering can alter. 
Thus the statement is correct in this respect. However, if we define "progress" more narrowly--in terms of economic standard of living and comfort level--recent technological innovations have indeed  brought about clear progress for humanity.





0_1,1_1,2_1,3_1,4_1,5_1,6_1,7_1,8_1,9_1,10_1,11_-1,12_1,13_1,14_1,15_1

Do we need careful measurements and logic to determine whether and to what extent we are progressing or regressing? 
I agree that in certain endeavors quantitative measurements and logical analysis of data are essential for this purpose.
 However, in other realms objective data provides little guidance for determining progress. My view applies to individuals as well as society as a whole.

As for monitoring individual progress, the extent to which careful measurement and logical analysis of data are required depends on the specific endeavor. 
In the area of personal finance, objective measurements are critical.
 We might feel that we are advancing financially when we buy a new car or a better home, or when our salary increases. 
Yet these signs of personal economic success can be deceptive. 
Cars depreciate quickly in value, and residential real estate must appreciate steadily to offset ownership expenses.
 Even a pay raise is no sure sign of personal financial progress; if the raise fails to keep pace with the cost of living then the real salary is actually in decline.

In the area of one's physical well-being, however, quantitative measurement might be useful yet insufficient. 
Quantitative data such as blood pressure, cholesterol level, and body weight are useful objective indicators of physical health. 
Yet quantitative measurement and logic can only take us so far when it comes to physical well-being. 
Levels of physical discomfort and pain, the most reliable indicators of physical well-being, cannot be quantified. 
And of course our emotional and psychological well-being, which can have a profound impact on our physical health, defy objective measurement altogether.

On a societal level, as on a personal level, the extent to which careful measurement and logic are needed to determine progress depends on the endeavor. 
In macro-economics, as in personal finance, objective measurements are critical. For example, a municipality, state, or nation might sense that things are improving economically when its rate of unemployment declines. 
Yet if new jobs are in poor-paying positions involving unskilled labor, this apparent advance might actually be a retreat. 
And, a boom in retail sales might amount to regress if the goods sold are manufactured by foreign firms, who benefit from the boom at the expense of
domestic business expansion.
 Technological progress also requires carefu measurement. 
Advances is computer technology can only be determined by such factors as processing and transfer speeds, numbers of in staUations and users, amounts of data accessed, and so forth. 
And, advances in biotechnology are determined by statistical measurements of the effectiveness of new drugs and other treatments, and by demographic statistics regarding the incidence of the ailments that the technology seeks to ameliorate.

In contrast, socio-political progress is less susceptible to objective measurement. For instance, progress in social welfare might be measured by the number of homeless people, incidence of domestic violence, or juvenile crime rate. 
Yet would an increase in the number of single mothers on welfare indicate that our society is becoming more compassionate and effective in helping its victims, or would it indicate regress by showing that our private sector and education systems are failing? 
Moreover, when it comes to our legal system and to politics, progress has little to do with numbers, or even logic.
 For example, to what extent, if any, would more lenient gun ownership laws indicate progress, considering the competing interests of individual freedom and public safety? 
Do anti-abortion laws indicate a sociological advance or retreat? 
Or, when a political party gains greater control of a legislature by sweeping a particular election, is this progress or regress?


In sum, although the statement has merit, it unfairly generalizes.
 In areas such as finance, economics, and computing technology, all of which involve nothing but quantifiable data, nothing but careful measurement and logic suffice to determine the extent of progress. 
In other areas, such as health care and social welfare, determining progress requires both objective measurement and subjective judgment.
Finally, progress in politics and law is an entirely subjective matter--depending on each individual's values, priorities, and interests.